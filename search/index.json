[{"content":"Setup Monitoring for your homelab environment In this guide, we\u0026rsquo;ll set up monitoring tools to oversee the operation of your homelab, specifically for Proxmox, VMs, and Docker containers.\nSetup InfluxDB We\u0026rsquo;ll begin by setting up monitoring for the core of the infrastructure, the hypervisor.\nFollow the docker-compose.yml and .env file configuration to set up your InfluxDB container:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 version: \u0026#39;3.9\u0026#39; services: # Influx Database influxdb: container_name: influxdb image: influxdb:2.7.1-alpine restart: unless-stopped # Custom DNS for container here: #dns: # - 1.1.1.1 ports: - \u0026#39;8086:8086\u0026#39; volumes: - \u0026#39;./influxdb-data:/var/lib/influxdb2\u0026#39; - \u0026#39;./etc:/etc/influxdb2\u0026#39; # # If you\u0026#39;re using self-signed certs, uncomment the lines below # - /etc/ssl/cert.pem/:/etc/ssl/cert.pem # - /etc/ssl/cert-key.pem/:/etc/ssl/cert-key.pem # command: influxd --tls-cert=/etc/ssl/cert.pem --tls-key=/etc/ssl/cert-key.pem environment: - DOCKER_INFLUXDB_INIT_MODE=setup - DOCKER_INFLUXDB_INIT_USERNAME=${INFLUX_USER} - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUX_PASS} - DOCKER_INFLUXDB_INIT_ORG=${INFLUX_ORG} - DOCKER_INFLUXDB_INIT_BUCKET=${INFLUX_BUCKET} - DOCKER_INFLUXDB_INIT_RETENTION=1w #configure data retention by week count # The authentication token to associate with the system\u0026#39;s initial super-user. If not set, a token will be auto-generated by the system. # - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=my-super-secret-auth-token # Traefik Example: #labels: # - \u0026#34;traefik.enable=true\u0026#34; # - \u0026#34;traefik.http.routers.influx.entrypoints=http\u0026#34; # - \u0026#34;traefik.http.routers.influx.rule=Host(`influx.cinderblock.tech`)\u0026#34; # - \u0026#34;traefik.http.middlewares.influx-https-redirect.redirectscheme.scheme=https\u0026#34; # - \u0026#34;traefik.http.routers.influx.middlewares=influx-https-redirect\u0026#34; # - \u0026#34;traefik.http.routers.influx-secure.entrypoints=https\u0026#34; # - \u0026#34;traefik.http.routers.influx-secure.rule=Host(`influx.cinderblock.tech`)\u0026#34; # - \u0026#34;traefik.http.routers.influx-secure.tls=true\u0026#34; # - \u0026#34;traefik.http.routers.influx-secure.service=influx\u0026#34; # - \u0026#34;traefik.http.services.influx.loadbalancer.server.port=8086\u0026#34; # - \u0026#34;traefik.docker.network=proxy\u0026#34; networks: - proxy - monitoring # I\u0026#39;ll be using Proxy for my external connection, and monitoring for db to db network communication networks: proxy: external: true monitoring: external: false Next, set your environment variables either in your environment or with a .env file in the same folder.\n1 2 3 4 INFLUX_USER= INFLUX_PASS= INFLUX_ORG= INFLUX_BUCKET= Spin it up!\nSetup Proxmox Data API After setting up and running InfluxDB (secured behind an SSL certificate), go to your Proxmox instance.\nNavigate to Datacenter --\u0026gt; Metric Server --\u0026gt; Add --\u0026gt; InfluxDB and provide the necessary information for your InfluxDB server.\nYou will get the token from the dashboard of your influxdb. Navigate to Load Data --\u0026gt; API Token\nNote: If your Proxmox instance does not have a CA certificate, you\u0026rsquo;ll encounter an SSL error. To resolve this error, you can use Cloudflare to get your CA certificate, and then install it on Proxmox.\nIf your network uses a proxy that forwards all HTTP requests to HTTPS (like Traefik), you can simply set the protocol to HTTP, and Traefik will handle the rest. This allows you to skip the next part about getting a Cloudflare CA.\nGetting your Cloudflare CA Navigate to Cloudflare, go into your \u0026lsquo;Website\u0026rsquo; and drop down into SSL/TLS --\u0026gt; Origin Server. Create a CA using RSA. Use the resulting CA and CA-Key to create respective files:\nca-key.pem ca.pem Now, we are going to copy the needed CA to Proxmox to allow it to send metrics to our Influx database.\n1 scp `proxmox-user`@`proxmox-url`:/usr/local/share/ca-certificates/ca.crt Then ssh into your Proxmox server, and update the CA-Certificates\n1 update-ca-certificates Setup Grafana for data visbility Now that we have InfluxDB setup, and Proxmox is sending its data there, lets setup the visual part of the process.\ncreate your docker-compose.yml file and input the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 version: \u0026#39;3.9\u0026#39; services: grafana: image: grafana/grafana:10.0.1 user: \u0026#34;0\u0026#34; # Set this to prevent permission errors during Grafana deployment container_name: grafana-monitor #dns: # - 1.1.1.1 ports: - \u0026#34;3020:3000\u0026#34; volumes: - ./grafana-data:/var/lib/grafana #environment: #- \u0026#34;GF_SERVER_ROOT_URL=http://grafana.cinderblock.tech/\u0026#34; #- GF_INSTALL_PLUGINS=grafana-clock-panel #- GF_DEFAULT_INSTANCE_NAME=homelab-monitor #- GF_SECURITY_ADMIN_USER=austin #- GF_AUTH_GOOGLE_CLIENT_SECRET=${GRAF_SECRET} #- GF_PLUGIN_GRAFANA_IMAGE_RENDERER_RENDERING_IGNORE_HTTPS_ERRORS=true #- GF_FEATURE_TOGGLES_ENABLE=newNavigation restart: unless-stopped # Example Traefik config #labels: #- \u0026#34;traefik.enable=true\u0026#34; #- \u0026#34;traefik.http.routers.grafana-monitor.entrypoints=http\u0026#34; #- \u0026#34;traefik.http.routers.grafana-monitor.rule=Host(`grafana-monitor.cinderblock.tech`)\u0026#34; #- \u0026#34;traefik.http.middlewares.grafana-monitor-https-redirect.redirectscheme.scheme=https\u0026#34; #- \u0026#34;traefik.http.routers.grafana-monitor.middlewares=grafana-monitor-https-redirect\u0026#34; #- \u0026#34;traefik.http.routers.grafana-monitor-secure.entrypoints=https\u0026#34; #- \u0026#34;traefik.http.routers.grafana-monitor-secure.rule=Host(`grafana-monitor.cinderblock.tech`)\u0026#34; #- \u0026#34;traefik.http.routers.grafana-monitor-secure.tls=true\u0026#34; #- \u0026#34;traefik.http.routers.grafana-monitor-secure.service=grafana-monitor\u0026#34; #- \u0026#34;traefik.http.services.grafana-monitor.loadbalancer.server.port=3000\u0026#34; #- \u0026#34;traefik.docker.network=proxy\u0026#34; networks: - proxy - monitoring networks: proxy: external: true monitoring: external: false Spin it up and connect to the URL to sign in. Default username and password are both admin.\nAdd InfluxDB datasource into Grafana Now that we have both containers working, navigate to the webpage for Grafana. From here, go into administration --\u0026gt; data sources\nAdd a source, select InfluxDB.\nChange Query language to Flux Enter your https://FQDN Basic Auth: User + Pass for Influx Bucket and organization for Influx Another API Token from Influx Save and test, ensure that you can see buckets from Influx! Visualize the data! For this, there is a grafana plugin called Proxmox [Flux]. This\u0026rsquo;ll do the trick here, since we are in this case, importing Proxmox data.\nNavigate to the Proxmox [Flux] page, and pull the import ID At the top of your grafana page, hit the +, import dashboard, and throw that copied import ID in there. From here navigate to your dashboard for InfluxDB, change the Bucket selected at the top, and you\u0026rsquo;ve got a visualized dashboard for your Proxmox data!\nUseful Resources Here are some useful resources for further exploration:\nTraefik site My GitHub Repository Grafana InfluxDB ","date":"2023-06-28T00:00:00Z","image":"https://cinderblock.github.io/p/monitor-proxmox-grafana-influxdb/proxmox-influxdb-monitor_huc8d5dcc7a73e9fb21f17ddd7cdc67105_933394_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/monitor-proxmox-grafana-influxdb/","title":"Monitor Proxmox - Grafana - InfluxDB"},{"content":"Using Rancher and Traefik to provide certificates and a management platform to a K3S Cluster In this guide, we will be creating an environment consisting of Rancher, Traefik, MySQL, and a K3S cluster. By setting it up this way, we can automate certificate management through Cloudflare (For more details about Cloudflare certificates behind Traefik, refer to other posts, as this won\u0026rsquo;t be covered here.)\nPrerequisites To get started, you\u0026rsquo;ll need:\nFour Virtual Machines (VMs). In this guide, I\u0026rsquo;m using VMs running Ubuntu Server 22.04. One VM will host our Docker containers, and the other three will serve as our K3S nodes. Familiarity with Docker and Docker-compose. Basic understanding of load balancing and networking. An internal DNS server for name resolution. Let\u0026rsquo;s get started.\nSetting up Traefik Our first task is to set up our Traefik container, which will provide TLS certificate management for Rancher and the K3S nodes. We need to create directories to host the necessary files.\nCreate the diretories to host the files, and the files reqruied.\n1 2 3 mkdir traefik mkdir traefik/data/ touch ./traefik/docker-compose.yml ./traefik/data/traefik.yml ./traefik/data/config.yml Here are the contents for the respective files:\ndocker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 version: \u0026#39;3\u0026#39; services: traefik: image: traefik:latest container_name: traefik restart: unless-stopped security_opt: - no-new-privileges:true networks: - proxy ports: - \u0026#34;80:80\u0026#34; #HTTP - \u0026#34;443:443\u0026#34; #HTTPS environment: - CF_API_EMAIL=Cloudflare-email-here - CF_API_KEY=Cloudflare-api-key-here volumes: - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro - ./logs/traefik-access.log:/var/log/traefik - ./data/traefik.yml:/traefik.yml:ro - ./data/acme.json:/acme.json - ./data/config.yml:/config.yml:ro #- ./.htpasswd:/auth/.htpasswd labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`FQDN-Traefik-Here`)\u0026#34; - \u0026#34;traefik.http.middlewares.traefik-auth.basicauth.users=${BASICAUTHUSER}\u0026#34; # Take note, using a $ in the .env file, will double the amount of \u0026#39;$\u0026#39; signs being pulled over. # Double check config variables with \u0026#39;docker-compose config\u0026#39; to see .env results # Generate BASID_AUTH_PASS: echo $(htpasswd -nb \u0026#34;\u0026lt;USER\u0026gt;\u0026#34; \u0026#34;\u0026lt;PASSWORD\u0026gt;\u0026#34;) | sed -e s/\\\\$/\\\\$\\\\$/g - \u0026#34;traefik.http.middlewares.traefik-https-redirect.redirectscheme.scheme=https\u0026#34; - \u0026#34;traefik.http.middlewares.sslheader.headers.customrequestheaders.X-Forwarded-Proto=https\u0026#34; - \u0026#34;traefik.http.routers.traefik.middlewares=traefik-https-redirect\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.rule=Host(`FQDN-Traefik-Here`)\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.middlewares=traefik-auth\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls=true\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls.certresolver=cloudflare\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls.domains[0].main=Domain.com\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls.domains[0].sans=*.Domain.com\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.service=api@internal\u0026#34; networks: proxy: external: true traefik.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 api: dashboard: true debug: true entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: \u0026#34;https\u0026#34; scheme: \u0026#34;https\u0026#34; https: address: \u0026#34;:443\u0026#34; k3s: address: \u0026#34;:6443\u0026#34; serversTransport: insecureSkipVerify: true providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: false file: filename: /config.yml certificatesResolvers: cloudflare: acme: email: ${CF_API_EMAIL} storage: acme.json dnsChallenge: provider: cloudflare #disablePropagationCheck: true # uncomment this if you have issues pulling certificates through cloudflare, By setting this flag to true disables the need to wait for the propagation of the TXT record to all authoritative name servers. resolvers: - \u0026#34;1.1.1.1:53\u0026#34; - \u0026#34;1.0.0.1:53\u0026#34; log: level: INFO filePath: \u0026#34;/var/log/traefik/traefik.log\u0026#34; accessLog: filePath: \u0026#34;/var/log/traefik/access.log\u0026#34; config.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 http: #region routers routers: # This is an extremely simplified Traefik configuration file, as the HTTP portion is empty since Rancher will be setting up its # Labeling structure within the docker-container. See my GitHub for a more advanced version of this config file. #region services services: #endregion middlewares: # Redirect all http traffic to https https-redirectscheme: redirectScheme: scheme: https permanent: true # Add default headers for redirection default-headers: headers: frameDeny: true browserXssFilter: true contentTypeNosniff: true forceSTSHeader: true stsIncludeSubdomains: true stsPreload: true stsSeconds: 15552000 customFrameOptionsValue: SAMEORIGIN customRequestHeaders: X-Forwarded-Proto: https # Whitelist all private IP addresses default-whitelist: ipWhiteList: sourceRange: - \u0026#34;10.0.0.0/8\u0026#34; - \u0026#34;192.168.0.0/16\u0026#34; - \u0026#34;172.16.0.0/12\u0026#34; # secured: chain: middlewares: - default-whitelist - default-headers tcp: routers: # k3s port k3s: entryPoints: k3s rule: \u0026#34;HostSNI(`*`)\u0026#34; service: k3s tls: {} middlewares: - \u0026#34;default-whitelist\u0026#34; # Load balancers - Services services: # k3s loadbalancer k3s: loadBalancer: servers: - address: \u0026#34;192.168.1.235:6443\u0026#34; - address: \u0026#34;192.168.1.236:6443\u0026#34; - address: \u0026#34;192.168.1.237:6443\u0026#34; # Whitelist all private IP addresses middlewares: default-whitelist: ipWhiteList: sourceRange: - \u0026#34;10.0.0.0/8\u0026#34; - \u0026#34;192.168.0.0/16\u0026#34; - \u0026#34;172.16.0.0/12\u0026#34; If you need clarity or assistance in setting up Cloudflare to hand out certificates for Traefik, there are plenty of guides out there! Then start up the docker-compose file.yml from the root of the ./traefik folder\n1 docker-compose up -d Setting up Rancher We will now create a directory for Rancher and place a Docker-compose.yml file inside it.\n1 2 mkdir rancher nano ./rancher/docker-compose.yml Fill the docker-compose with:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 version: \u0026#39;3.9\u0026#39; services: rancher: # --no-cascerts allows for a wildcard cert to be applied, comment out if you will not be using an assigned certificate. command: \u0026#39;--no-cacerts\u0026#39; image: \u0026#39;rancher/rancher:latest\u0026#39; #dns: #Put your DNS servers below # - 192.168.1.253 # - 1.1.1.1 privileged: true ports: - \u0026#39;9443:443\u0026#39; - \u0026#39;9080:80\u0026#39; networks: - proxy volumes: - ./opt/rancher:/var/lib/rancher restart: unless-stopped # If not using Traefik, comment this out for your SSL/TLS solution. labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.rancher.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.rancher.rule=Host(`rancher.cinderblock.tech`)\u0026#34; - \u0026#34;traefik.http.middlewares.rancher-https-redirect.redirectscheme.scheme=https\u0026#34; - \u0026#34;traefik.http.routers.rancher.middlewares=rancher-https-redirect\u0026#34; - \u0026#34;traefik.http.routers.rancher-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.rancher-secure.rule=Host(`rancher.cinderblock.tech`)\u0026#34; - \u0026#34;traefik.http.routers.rancher-secure.tls=true\u0026#34; - \u0026#34;traefik.http.routers.rancher-secure.service=rancher\u0026#34; - \u0026#34;traefik.http.services.rancher.loadbalancer.server.port=80\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; networks: proxy: external: true After completing the setup, you can start it up. I highly recommend setting up an SSL/TLS certificate for Rancher, otherwise you\u0026rsquo;ll have to run it without a certificate, which is insecure.\n1 docker-compose up -d Setting up Mysql Next, we will create a MySQL Docker container. Start by creating a directory and a Docker-compose file in it.\n1 2 3 mkdir mysql cd mysql nano docker-compose.yml Fill the Docker-compose file with the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 version: \u0026#39;3.9\u0026#39; services: mysql: image: mysql:latest restart: unless-stopped container_name: k3s-mysql ports: - 3396:3306 expose: - \u0026#34;3396\u0026#34; volumes: - ./mysql-data:/var/lib/mysql/ - ./setup.sql:/docker-entrypoint-initdb.d/setup.sql environment: - MYSQL_ROOT_PASSWORD=${MYSQL_PASS} # Put your sql password here for Root access Setup a script file to automatically create the SQL database and uses:\n1 nano setup.sql Put the following SQL command into the setup.sql file, and ensure to change user, password, to respective pieces of information:\n1 2 3 4 CREATE DATABASE k3s COLLATE latin1_swedish_ci; CREATE USER \u0026#39;user\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; GRANT ALL ON k3s.* TO \u0026#39;user\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; Start it up!\n1 docker-compuse up -d Setting up the primary node. Ensure you have Ubuntu Server 22.04 setup and updated. Also ensure these few things are in order:\ncurl package is installed IPs are in order DNS is pointing correctly On the primary node, you will want to run the following:\n1 2 3 4 curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.25.9+k3s1 sh -s - server \\ --tls-san ${FQDN-load-balancer-HERE} \\ --datastore-endpoint=\u0026#34;mysql://${user}:${pass}@tcp(${IP-MYSQL}:3396)/${database-name}\u0026#34; # Replace \u0026#39;${}` with your values Once this installs successfully, run the following cat command on the node, to get the required token to setup the next two nodes to the same cluster:\n1 sudo cat /var/lib/rancher/k3s/server/node-token Copy that token, we will use it in the next part.\nSetting up the other two nodes. SSH into the next node. Run the following command to join it to the K3S cluster:\n1 2 3 4 curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.25.9+k3s1 K3S_URL=https://${FQDN-LOAD-BANACER-HERE}:6443 K3S_TOKEN=${TOKEN-FROM-PRIMARY-NODE-HERE} sh -s - server \\ --tls-san ${FQDN-load-balancer-HERE} \\ --datastore-endpoint=\u0026#34;mysql://${user}:${pass}@tcp(${IP-OF-MYSQL}:3396)/${database-name}\u0026#34; #Replace values \u0026#39;${}` with your values Repeat for the second node.\nOnce this is done, test it is working with\n1 sudo kubectl get nodes If the result shows the three nodes together, congragulations, you have a K3S cluster.\nConnect K3S cluster to rancher management panel Navigate to your Rancher Webportal, sign in, and go to the Cluster Management tab.\nSelect Import Existing Cluster Select Generic Give it a name, and hit create Copy the top command under Registration, and give rancher the ability to command Kubectl. Run it on all three k3s nodes. Then you are good and free to have Rancher manage your k3s cluster.\nUseful Resources Here are some useful resources for further exploration:\nTraefik site My GitHub Repository Rancher ","date":"2023-06-20T00:00:00Z","image":"https://cinderblock.github.io/p/rancher-behind-traefik-k3s/rancher-traefik_hu29572413a1d3efce2a3d392c81b7be5f_370473_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/rancher-behind-traefik-k3s/","title":"Rancher behind Traefik - K3S"},{"content":"Crowdsec \u0026amp; Traefik: Automating Security in Your Homelab Hello there! Today, I\u0026rsquo;m thrilled to introduce you to Crowdsec, an open-source Security threat intelligence project that\u0026rsquo;s perfect for boosting security in your homelab.\nPrerequisites Before we embark on our journey into a safer network, let\u0026rsquo;s check off these prerequisites:\nPrior experience with networking and a good understanding of network protocols. Some exposure to Docker as we\u0026rsquo;ll be using Docker-Compose for the setup. An external service in your homelab that you\u0026rsquo;re eager to fortify! Ready? Let\u0026rsquo;s dive in!\nSetting up Traefik Setting up the Docker container for Traefik is a breeze, provided you have a couple of files handy.\nNote: In this setup, we\u0026rsquo;ll be using Cloudflare as the certificate handler via Let\u0026rsquo;s Encrypt. Although I won\u0026rsquo;t go into details on this here, you\u0026rsquo;ll need a domain and API access to your DNS Zones.\nFirst, let\u0026rsquo;s create a directory named traefik and within it, two more directories, logs/ and data/.\n1 2 3 mkdir traefik cd traefik mkdir logs/ data/ Now, in the traefik directory, create a new docker-compose.yml file.\n1 nano docker-comopse.yml Input the following configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 version: \u0026#39;3\u0026#39; services: traefik: image: traefik:latest container_name: traefik restart: unless-stopped security_opt: - no-new-privileges:true networks: - proxy ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; environment: - CF_API_EMAIL=${CF_API_EMAIL} - CF_API_KEY=${CF_API_KEY} volumes: - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro - ./logs/traefik-access.log:/var/log/traefik - ./data/traefik.yml:/traefik.yml:ro - ./data/acme.json:/acme.json - ./data/config.yml:/config.yml:ro labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`traefik.domain.com`)\u0026#34; #Domain-Here - \u0026#34;traefik.http.middlewares.traefik-auth.basicauth.users=${BASICAUTHUSER}\u0026#34; # Generate BASID_AUTH_PASS: echo $(htpasswd -nb \u0026#34;\u0026lt;USER\u0026gt;\u0026#34; \u0026#34;\u0026lt;PASSWORD\u0026gt;\u0026#34;) | sed -e s/\\\\$/\\\\$\\\\$/g - \u0026#34;traefik.http.middlewares.traefik-https-redirect.redirectscheme.scheme=https\u0026#34; - \u0026#34;traefik.http.middlewares.sslheader.headers.customrequestheaders.X-Forwarded-Proto=https\u0026#34; - \u0026#34;traefik.http.routers.traefik.middlewares=traefik-https-redirect\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.rule=Host(`traefik.domain.com`)\u0026#34; #Domain-Here - \u0026#34;traefik.http.routers.traefik-secure.middlewares=traefik-auth\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls=true\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls.certresolver=cloudflare\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls.domains[0].main=.domain.com\u0026#34; #Domain-Here - \u0026#34;traefik.http.routers.traefik-secure.tls.domains[0].sans=*..domain.com\u0026#34; #Domain-Here - \u0026#34;traefik.http.routers.traefik-secure.service=api@internal\u0026#34; networks: proxy: external: true #Ensure to setup docker netowork (`docker network create proxy`) With that created, navigate into your data/ directory and create config.yml\n1 2 cd data/ nano config.yml Fill it with the following: (This will be updated again later; I left a snippet in there about pi-hole, for refernence on how to setup a HTTP / HTTPS router service)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 http: #region routers routers: # Pihole http pihole: entryPoints: - \u0026#34;http\u0026#34; rule: \u0026#34;Host(`pihole.domain.com`)\u0026#34; middlewares: - https-redirectscheme service: pihole # Pihole https pihole-secure: entryPoints: - \u0026#34;https\u0026#34; rule: \u0026#34;Host(`pihole.domain.com`)\u0026#34; middlewares: - default-headers - addprefix-admin tls: {} service: pihole #region services services: passHostHeader: true # Pihole Loadbalancer pihole: loadBalancer: servers: - url: \u0026#34;http://pihole-ip-here:80\u0026#34; passHostHeader: true #endregion middlewares: # Prefix all traffic to admin page addprefix-admin: addPrefix: prefix: \u0026#34;/admin\u0026#34; # Redirect all http traffic to https https-redirectscheme: redirectScheme: scheme: https permanent: true # Add default headers for redirection default-headers: headers: frameDeny: true browserXssFilter: true contentTypeNosniff: true forceSTSHeader: true stsIncludeSubdomains: true stsPreload: true stsSeconds: 15552000 customFrameOptionsValue: SAMEORIGIN customRequestHeaders: X-Forwarded-Proto: https # Whitelist all private IP addresses default-whitelist: ipWhiteList: sourceRange: - \u0026#34;10.0.0.0/8\u0026#34; - \u0026#34;192.168.0.0/16\u0026#34; - \u0026#34;172.16.0.0/12\u0026#34; # Used for headers secured: chain: middlewares: - default-whitelist - default-headers tcp: routers: # Whitelist all private IP addresses middlewares: default-whitelist: ipWhiteList: sourceRange: - \u0026#34;10.0.0.0/8\u0026#34; - \u0026#34;192.168.0.0/16\u0026#34; - \u0026#34;172.16.0.0/12\u0026#34; While still in the same directory, create traefik.yml\n1 nano traefik.yml and fill it with:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 api: dashboard: true debug: true entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: \u0026#34;https\u0026#34; scheme: \u0026#34;https\u0026#34; https: address: \u0026#34;:443\u0026#34; serversTransport: insecureSkipVerify: true providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: false file: filename: /config.yml certificatesResolvers: cloudflare: acme: email: ${CF_API_EMAIL} # Email of your cloudflare account here storage: acme.json dnsChallenge: provider: cloudflare resolvers: - \u0026#34;1.1.1.1:53\u0026#34; - \u0026#34;1.0.0.1:53\u0026#34; log: level: INFO filePath: \u0026#34;/var/log/traefik/traefik.log\u0026#34; accessLog: filePath: \u0026#34;/var/log/traefik/access.log\u0026#34; Now navigate back to the root ./traefik folder, and re-create the container\n1 2 cd ../ docker-compose up -d --force-recreate Testing Traefik Don\u0026rsquo;t take any further steps until you are certain traefik is working. Check the the log file, ./traefik/logs/traefik.log with tail -f traefik.log for errors, and work from there.\nSetting up Crowdsec With our traefik container running and doing all of its proxy magic, now we can setup our security item. Crowdsec. navigate back to your root folder beneath ./traefik and make a new directory titled crowdsec, with a config/ folder, and a docker-compose.yml file.\n1 2 3 4 mkdir crowdsec cd crowdsec mkdir config nano docker-compose.yml Within this docker-compose file put the following for now:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 version: \u0026#39;3.8\u0026#39; services: crowdsec: image: crowdsecurity/crowdsec:latest container_name: crowdsec environment: GID: \u0026#34;${GID-1000}\u0026#34; COLLECTIONS: \u0026#34;crowdsecurity/linux crowdsecurity/traefik\u0026#34; volumes: - ./config/acquis.yaml:/etc/crowdsec/acquis.yaml - ./db:/var/lib/crowdsec/data/ - ./config:/etc/crowdsec/ - ../traefik/logs/traefik-access.log:/var/log/traefik/:ro # This volume points back to the traefik logs we are writing and storing locally ports: - \u0026#34;8083:8080\u0026#34; networks: - proxy restart: unless-stopped networks: proxy: external: true Next, create config/acquis.yaml. This houses the config telling Crowdsec to look for the Traefik Logs.\n1 nano config/acquis.yml (NOTE: This must be a .yaml and not .yml) Fill it with the following config:\n1 2 3 4 5 6 7 8 9 filenames: - /var/log/traefik/* labels: type: traefik --- filenames: - /var/log/auth.log labels: type: syslog While in ./crowdsec, spin it up\n1 `docker-compose up -d --force-recreate` Testing Crowdsec Again, it is critical you save yourself some time and ensure the service is working as intended before moving forward. Trust me.\nCheck first with:\n1 `docker exec crowdsec cscli metrics` Search for the top of the result, and check that crowdsec can see and is parsing files from /var/log/traefik/\nIf it is, you can continue! Otherwise look back and just double check syntax and what not.\nIntegrating Crowdsec and Traefik Run a couple of commands into the crowdsec docker container to update it, and bring versioning into line of our collections in use from the docker-compose file, \u0026lsquo;crowdsecurity/linux\u0026rsquo; \u0026lsquo;crowdsecurity/traefik\u0026rsquo;\n1 docker exec crowdsec cscli hub update \u0026amp;\u0026amp; docker exec crowdsec cscli hub upgrade If you wish, you can automate this, to ensure constant updates between the local crowdsec, and the remote updates from the repos. I\u0026rsquo;ll be keeping it up-to-date by using ansible-semaphore.\nAutomating Updates with Cron Run the crontab -e command. Select 1 for /bin/nano, and at the bottom put in\n1 0 0,6,12,18 * * * docker exec crowdsec cscli hub update \u0026amp;\u0026amp; docker exec crowdsec cscli hub upgrade This will update and upgrade the hub every 6 hours.\nEnhancing Crowdsec with a Bouncer We will be using a bouncer designed for traefik (Shout out to: fbonalair for creating this: https://github.com/fbonalair/traefik-crowdsec-bouncer).\nGo back into your traefik folder, and nano docker-compose.yml, add the following to your file to have it start up right after your crowdsec container:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 bouncer-traefik: image: docker.io/fbonalair/traefik-crowdsec-bouncer:latest container_name: bouncer-traefik environment: CROWDSEC_BOUNCER_API_KEY: ${bouncer-api-key} # Get this api key by running `docker exec crowdsec cscli bouncers add bouncer-traefik` CROWDSEC_AGENT_HOST: crowdsec:8080 GIN_MODE: release ports: - \u0026#34;8082:8080\u0026#34; networks: - proxy # same network as traefik + crowdsec depends_on: - crowdsec restart: unless-stopped Once you have your API key in the file, run docker-compouse up -d --force-recreate and have it create the bouncer.\nUpdate Traefik to support the bouncer Now, navigate back into the Traefik folder and edit the data/config.yml file. Add the following to the http: -\u0026gt; services: -\u0026gt; middlewares: section:\n1 2 3 4 crowdsec-bouncer: forwardauth: address: http://http://bouncer-traefik:8080/api/v1/forwardAuth trustForwardHeader: true You must also add this middleware to your config/traefik.yml file, causing the redirect out to the bouncer at the time of access:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 api: dashboard: true debug: true entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: \u0026#34;https\u0026#34; scheme: \u0026#34;https\u0026#34; https: address: \u0026#34;:443\u0026#34; http: middlewares: - \u0026#34;crowdsec-bouncer@file\u0026#34; With that done, run docker-compose up -d --force-recreate for traefik.\nReaping the Rewards: Enrolling CrowdSec Now that crowdsec is working, lets enroll our client. This will add benefits with a overall a nice dashboard to manage it\nEasily monitoring alerts Seeing activity going on Bounces occuring Navigate to CrowdSec\u0026rsquo;s website Create an account. Search for \u0026lsquo;Enroll your crowdsec security engine!\u0026rsquo; and copy the code at the end of it (see picture below). Back on your Crowdsec host, run 1 docker exec crowdsec cscli console enroll code-pasted-here Once pasted, run 1 docker restart crowdsec Go back to CrowdSec\u0026rsquo;s website and accept the instance Enjoy your management dashboard Notable benefits of Enrolling: You can receive notifications when your bouncers get updates. You have the ability to sync your scenarios across all of your bouncers, making management a breeze. Exploring More Interested in taking your security measures to another level?\nFigure out how to add Authelia behind your crowdsec and traefik containers to enhance security a step further with multi-factor web access for your apps. More info here. Check out this plugin for Traefik: Crowdsec Bouncer Traefik Plugin Useful Crowdsec commands To easily execute the following commands, you can use this to exec into your crowdsec container.\n1 docker exec -it crowdsec /bin/bash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # See all parsers and scenarios deployed cscli hub list # See all decisions that have occured cscli decisions list # See all alerts at a glance cscli alerts list # Block a specific IP cscli decisions add --ip 1.2.3.4 # Unblock a specific IP cscli decisions delete -i 1.2.3.4 Useful Resources Here are some useful resources for further exploration:\nCrowdsec site Traefik site traefik-crowdsec-bouncer github My GitHub Repository IBRACORP ","date":"2023-06-09T00:00:00Z","image":"https://cinderblock.github.io/p/crowdsec-traefik-automating-security-in-your-homelab/crowdsec-traefik_huaa4fe9ac5e5053e41472d3609373499b_395241_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/crowdsec-traefik-automating-security-in-your-homelab/","title":"Crowdsec \u0026 Traefik: Automating Security in Your Homelab"},{"content":"Overview In this blog post, I\u0026rsquo;d like to introduce you to Semaphore, an open-source Ansible management project. With Semaphore, you can automate and keep track of all your Ansible tasks in your homelab. Check out my current Ansible Playbooks in this GitHub repository.\nPrerequisites Before we dive into Semaphore, let\u0026rsquo;s ensure you meet the following prerequisites:\nSome prior experience with Ansible and understanding of playbook formatting. Ideally, some Docker experience as we\u0026rsquo;ll be using Docker-Compose for the setup. Something in your homelab that you want to manage! Getting the Docker Container Setup Setting up the Docker container for Semaphore is straightforward. You\u0026rsquo;ll need to configure two primary files.\nThe first file is docker-compose.yml, which contains all the configuration for the container. You can find the file in this repository. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 services: mysql: restart: unless-stopped ports: - 3306:3306 image: mysql:8.0 hostname: mysql-semaphore volumes: - semaphore-mysql:/var/lib/mysql environment: MYSQL_RANDOM_ROOT_PASSWORD: \u0026#39;yes\u0026#39; MYSQL_DATABASE: semaphore MYSQL_USER: ${MYSQL_USER} MYSQL_PASSWORD: ${MYSQL_PASS} networks: - proxy semaphore: restart: unless-stopped ports: - 3000:3000 image: semaphoreui/semaphore:latest environment: SEMAPHORE_DB_USER: ${MYSQL_USER} SEMAPHORE_DB_PASS: ${MYSQL_PASS} SEMAPHORE_DB_HOST: mysql-semaphore SEMAPHORE_DB_PORT: 3306 SEMAPHORE_DB_DIALECT: mysql SEMAPHORE_DB: semaphore SEMAPHORE_PLAYBOOK_PATH: /tmp/semaphore/ SEMAPHORE_ADMIN_PASSWORD: ${SEMA_ADMIN_PASS} SEMAPHORE_ADMIN_NAME: ${SEMA_ADMIN_USER} SEMAPHORE_ADMIN_EMAIL: ${SEMA_ADMIN_EMAIL} SEMAPHORE_ADMIN: ${SEMA_ADMIN_USER} SEMAPHORE_ACCESS_KEY_ENCRYPTION: ${SEMA_ACCESS_KEY} # Generate using command \u0026#39;head -c32 /dev/urandom | base64\u0026#39; #SEMAPHORE_LDAP_ACTIVATED: \u0026#39;no\u0026#39; #SEMAPHORE_LDAP_HOST: dc01.local.example.com #SEMAPHORE_LDAP_PORT: \u0026#39;636\u0026#39; #SEMAPHORE_LDAP_NEEDTLS: \u0026#39;yes\u0026#39; #SEMAPHORE_LDAP_DN_BIND: \u0026#39;uid=bind_user,cn=users,cn=accounts,dc=local,dc=shiftsystems,dc=net\u0026#39; #SEMAPHORE_LDAP_PASSWORD: \u0026#39;ldap_bind_account_password\u0026#39; #SEMAPHORE_LDAP_DN_SEARCH: \u0026#39;dc=local,dc=example,dc=com\u0026#39; #SEMAPHORE_LDAP_SEARCH_FILTER: \u0026#34;(\\u0026(uid=%s)(memberOf=cn=ipausers,cn=groups,cn=accounts,dc=local,dc=example,dc=com))\u0026#34; depends_on: - mysql networks: - proxy labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.semaphore.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.semaphore.rule=Host(`${DNS_HOSTNAME_CLIENT}`)\u0026#34; - \u0026#34;traefik.http.middlewares.semaphore-https-redirect.redirectscheme.scheme=https\u0026#34; - \u0026#34;traefik.http.routers.semaphore.middlewares=semaphore-https-redirect\u0026#34; - \u0026#34;traefik.http.routers.semaphore-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.semaphore-secure.rule=Host(`${DNS_HOSTNAME_CLIENT}`)\u0026#34; - \u0026#34;traefik.http.routers.semaphore-secure.tls=true\u0026#34; - \u0026#34;traefik.http.routers.semaphore-secure.service=semaphore\u0026#34; - \u0026#34;traefik.http.services.semaphore.loadbalancer.server.port=3000\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; networks: proxy: external: true volumes: semaphore-mysql: If you do not use Traefik for proxies, on the same host, go ahead and remove / comment out that part of the configuration).\nThe second file is a .env file that stores the credentials required for the Docker-Compose file. Fill in your valuable super-secret credentials in this file before spinning up the container. 1 2 3 4 5 6 7 MYSQL_PASS = MYSQL_USER = SEMA_ACCESS_KEY = SEMA_ADMIN_EMAIL = SEMA_ADMIN_PASS = SEMA_ADMIN_USER = DNS_HOSTNAME_CLIENT = Semaphore Configuration to Get Started Before you can start deploying Ansible tasks with Semaphore, there are a few necessary setup steps. In this tutorial, I will demonstrate the setup using my GitHub repository.\nSet up the \u0026lsquo;Key Store\u0026rsquo; profiles: Once you\u0026rsquo;re logged into the Ansible Semaphore site, navigate to the \u0026lsquo;Key Store\u0026rsquo; section. Create three keys: Key titled \u0026lsquo;None\u0026rsquo; with Type \u0026lsquo;None\u0026rsquo;. This will be used for GitHub repository access (since it\u0026rsquo;s public). Key titled \u0026lsquo;SSH-Key\u0026rsquo; with Type \u0026lsquo;SSH Key\u0026rsquo;. This will be used for Ansible to run without sudo. Key titled \u0026lsquo;SSH-Pass\u0026rsquo; with Type \u0026lsquo;Login with password\u0026rsquo;. This will be used for non-sudo SSH Ansible tasks. Key titled \u0026lsquo;SSH-Pass-Sudo\u0026rsquo; with Type \u0026lsquo;Login with Password\u0026rsquo;. This will be used for sudo-required Ansible SSH tasks. Set up the GitHub repository: Use the URL of the repository. Set the branch name (e.g., \u0026lsquo;main\u0026rsquo;). If it\u0026rsquo;s a public repository, set \u0026lsquo;Access Key\u0026rsquo; to None. If it\u0026rsquo;s an SSH private connection, create a key under \u0026lsquo;Key Store\u0026rsquo; for that key and select it here. Configure the inventory: Under \u0026lsquo;Inventory\u0026rsquo;, create a \u0026lsquo;New Inventory\u0026rsquo;. Provide a name and set the credentials to be used for non-sudo and sudo tasks (created earlier). For the Type, select File if you have a local file containing the inventory, or use Static if you want to manage the inventory within Semaphore. Create an environment file: Under \u0026lsquo;Environment\u0026rsquo;, create a \u0026lsquo;New Environment\u0026rsquo; named \u0026lsquo;default\u0026rsquo;. Leave the extra variables section as \u0026lsquo;{}\u0026rsquo;. Now that we have the initial required configuration done, let\u0026rsquo;s set up our playbooks.\nSemaphore Task Templates / Playbooks Navigate to the \u0026lsquo;Task Templates\u0026rsquo; tab, and we will set up our first Task playbook.\nCreate a \u0026lsquo;New Template\u0026rsquo; and provide the following parameters. Leave the Template as a \u0026lsquo;Task\u0026rsquo;: Name: \u0026lt;name\u0026gt; Description: \u0026lt;description\u0026gt; Playbook Filename: \u0026lt;file/location/within/repo\u0026gt; Inventory: \u0026lt;created-inv\u0026gt; Repository: \u0026lt;created-repo\u0026gt; Environment: \u0026lt;created-env\u0026gt; Now that the playbook is created, navigate into it, and hit \u0026lsquo;Run\u0026rsquo; You can use different features while executing the task typically foundational of Ansible such as, \u0026lsquo;Debug\u0026rsquo;,\u0026lsquo;Dry Run\u0026rsquo;, \u0026lsquo;Diff\u0026rsquo;, etc. From here, you\u0026rsquo;re all set! Start automating your Ansible tasks with Semaphore.\nUseful Resources Here are some useful resources for further exploration:\nAnsible Semaphore Site Ansible Documentation Docker Documentation My GitHub Repository ","date":"2023-05-24T00:00:00Z","image":"https://cinderblock.github.io/p/ansible-semaphore-automating-updates/Ansible-Semaphore_hu98170f7755439bd9ae64a73233c28b4d_1630110_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/ansible-semaphore-automating-updates/","title":"Ansible Semaphore - Automating Updates"},{"content":"Overview Welcome to the TackleBox, my repository that houses a diverse collection of projects and scripts.\nCheck out all the goodies in this repo on GitHub.\nA Quick Peek Greetings, fellow tech enthusiasts! I thought it would be a great idea to showcase the mishmash of projects and scripts I\u0026rsquo;ve accumulated in this repository of mine, aptly named the TackleBox. Within this repository, you\u0026rsquo;ll find a wide range of content, from network automation scripts using PowerShell and Ansible to infrastructure-as-code solutions with Packer and Terraform. I also explore various mini-projects involving Docker containers and dabble in the world of Kubernetes.\nAs someone who constantly seeks ways to streamline my workflow and simplify my life, I realized that the best approach would be to create a well-organized repository containing thorough documentation and useful scripts that I create along the way.\nSo please, take a look around, and you might discover something valuable: TackleBox - GitHub\n","date":"2023-05-01T00:00:00Z","image":"https://cinderblock.github.io/p/tacklebox-github-mega-repo/TackleboxGitHub_hub0f1b4139f31a5ae230b1c8057d73109_60397_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/tacklebox-github-mega-repo/","title":"TackleBox - GitHub Mega Repo"},{"content":"Overview A quick guide to setup and create an openvpn server that connects to a client to form a site-to-site. A solution to have a VPN into your home network without having a public IP available to you at home.\nRequirements Cloud VM with a public IP running an OpenVPN server Home VM/client running OpenVPN client Steps Create Linux VM in chosen cloud provider Setup Cloud VM OpenVPN Server Configure User Settings Configure VPN Settings Setup Home OpenVPN client Setup Client to forward IPv4 traffic and use NAT Automate connection at boot up Create Linux VM in chosen cloud provider It is important to remember to setup the firewall settings applied to the VPN to allow necessary ports. Port 22 for SSH and OpenVPN\u0026rsquo;s server port 943 are necessary.\nCheck out the Terraform configuration on GitHub You will need Azure CLI installed and setup, alongside Terraform for this to work For an automated Terraform deployment of a linux node in Azure, create the following files.\na Providers.tf file:\n1 2 3 4 5 6 7 8 9 10 11 12 # Base config terraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;\u0026gt;=3.0.0\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} } a networking.tf file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 # Create a resource group to maintain security settings along with network interfaces for VMs resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;vpn_server\u0026#34; { name = \u0026#34;myvpn-resources\u0026#34; location = \u0026#34;East US\u0026#34; } # ASSIGN ADDRESS SPACE TO RESOURCE GROUP resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vpn_server\u0026#34; { name = \u0026#34;vpn-server-network\u0026#34; address_space = [\u0026#34;192.168.0.0/16\u0026#34;] location = azurerm_resource_group.vpn_server.location resource_group_name = azurerm_resource_group.vpn_server.name } # ASSIGN SUBNET TO NETWORK ADDRESS SPACE resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;myvpn_subnet\u0026#34; { name = \u0026#34;vpnsubnet\u0026#34; resource_group_name = azurerm_resource_group.vpn_server.name virtual_network_name = azurerm_virtual_network.vpn_server.name address_prefixes = [\u0026#34;192.168.10.0/24\u0026#34;] } # Create public IP variable for Linux machine resource \u0026#34;azurerm_public_ip\u0026#34; \u0026#34;myvpn_public\u0026#34; { name = \u0026#34;myvpn-PublicIp\u0026#34; resource_group_name = azurerm_resource_group.vpn_server.name location = azurerm_resource_group.vpn_server.location allocation_method = \u0026#34;Static\u0026#34; } # ASSIGN NETWORK INTERFACE PER VM WE WILL BE USING resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;myvpn_linux\u0026#34; { name = \u0026#34;myvpn-nic\u0026#34; location = azurerm_resource_group.vpn_server.location resource_group_name = azurerm_resource_group.vpn_server.name ip_configuration { name = \u0026#34;internal\u0026#34; subnet_id = azurerm_subnet.myvpn_subnet.id private_ip_address_allocation = \u0026#34;Static\u0026#34; private_ip_address = var.myvpn_linux_priavte_ip public_ip_address_id = azurerm_public_ip.myvpn_public.id } } # Assignign network sec grp in Azure resource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;myvpn_linux\u0026#34; { name = \u0026#34;VPN-Ports\u0026#34; location = azurerm_resource_group.vpn_server.location resource_group_name = azurerm_resource_group.vpn_server.name #security_rule { #RDP # name = \u0026#34;RDP\u0026#34; # priority = 101 # direction = \u0026#34;Inbound\u0026#34; # access = \u0026#34;Allow\u0026#34; # protocol = \u0026#34;Tcp\u0026#34; # source_port_range = \u0026#34;*\u0026#34; # destination_port_range = \u0026#34;3389\u0026#34; # source_address_prefix = \u0026#34;*\u0026#34; # destination_address_prefix = \u0026#34;*\u0026#34; #} security_rule { #SSH name = \u0026#34;SSH\u0026#34; priority = 102 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;22\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { #HTTPS name = \u0026#34;HTTPS\u0026#34; priority = 103 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;443\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { #OpenVPN name = \u0026#34;OpenVPN\u0026#34; priority = 104 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Udp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;1194\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { #OpenVPN name = \u0026#34;OpenVPNsite\u0026#34; priority = 105 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;943\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } } # ASSIGN SECURITY GROUPS TO INTERFACES # LINUX SSH resource \u0026#34;azurerm_network_interface_security_group_association\u0026#34; \u0026#34;myvpn_linux\u0026#34; { network_interface_id = azurerm_network_interface.myvpn_linux.id network_security_group_id = azurerm_network_security_group.myvpn_linux.id } a outputs.tf file:\n1 2 3 4 5 6 output \u0026#34;Public_IP_Linux\u0026#34; { value = azurerm_public_ip.myvpn_public.ip_address } output \u0026#34;Private_IP_Linux\u0026#34; { value = azurerm_network_interface.myvpn_linux.private_ip_address } a 01-VPNserver.tf file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 resource \u0026#34;azurerm_linux_virtual_machine\u0026#34; \u0026#34;vpn\u0026#34; { name = var.linux_server resource_group_name = azurerm_resource_group.vpn_server.name location = azurerm_resource_group.vpn_server.location size = var.linux_vm_size admin_username = var.linux_username network_interface_ids = [ azurerm_network_interface.myvpn_linux.id ] admin_ssh_key { username = var.linux_username public_key = file(\u0026#34;${var.linux_ssh_key}\u0026#34;) } # Cloud-Init passed here custom_data = data.template_cloudinit_config.config.rendered os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = var.linux_sa_type } source_image_reference { publisher = var.linux_vm_os_publisher offer = var.linux_vm_os_offer sku = var.linux_vm_os_sku version = \u0026#34;latest\u0026#34; } depends_on = [azurerm_resource_group.vpn_server, azurerm_network_interface.myvpn_linux] } # Create cloud-init file to be passed into linux vm data \u0026#34;template_file\u0026#34; \u0026#34;user_data\u0026#34; { template = file(\u0026#34;./cloudinit_config.yml\u0026#34;) } # Render a multi-part cloud-init config making use of the part # above, and other source files data \u0026#34;template_cloudinit_config\u0026#34; \u0026#34;config\u0026#34; { gzip = true base64_encode = true # Main cloud-config configuration file. part { filename = \u0026#34;init.cfg\u0026#34; content_type = \u0026#34;text/cloud-config\u0026#34; content = \u0026#34;${data.template_file.user_data.rendered}\u0026#34; } } a variables.tf file:\n1 2 3 4 5 6 7 8 9 10 variable \u0026#34;linux_server\u0026#34; {} variable \u0026#34;linux_vm_os_publisher\u0026#34; {} variable \u0026#34;linux_vm_os_offer\u0026#34; {} variable \u0026#34;linux_vm_os_sku\u0026#34; {} variable \u0026#34;linux_vm_size\u0026#34; {} variable \u0026#34;linux_ssh_key\u0026#34; {} variable \u0026#34;linux_sa_type\u0026#34; {} variable \u0026#34;myvpn_linux_priavte_ip\u0026#34; {} variable \u0026#34;linux_username\u0026#34; {} variable \u0026#34;linux_password\u0026#34; {} a terraform.tfvars file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Azure Linux Server related params linux_vm_os_publisher = \u0026#34;Canonical\u0026#34; linux_vm_os_offer = \u0026#34;UbuntuServer\u0026#34; linux_vm_os_sku = \u0026#34;18.04-LTS\u0026#34; linux_vm_size = \u0026#34;Standard_B1s\u0026#34; linux_ssh_key = \u0026#34;/mnt/c/Users/austi/.ssh/id_rsa.pub\u0026#34; linux_sa_type = \u0026#34;Premium_LRS\u0026#34; # Which administrator password to set during vm customization linux_username = \u0026#34;austin\u0026#34; linux_password = \u0026#34;X$zz6AkcWMbkKF\u0026#34; # Naming Schemes linux_server = \u0026#34;Austin-VPN\u0026#34; # Networking Variables myvpn_linux_priavte_ip = \u0026#34;192.168.10.5\u0026#34; Finally, the cloudinit_config.yml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #cloud-config package_upgrade: true packages: - curl users: - name: vpn ssh-authorized-keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDA9eacR4LWgR5nceoXTmvcf+W8LUzIGCmDNrniaznFRmUuylRGRhPx3blclPB6ToK5bcUQAqTMfothgkB0Rr9xZW41zqqjMgefhyZXBPfEvplxuWTa4tTaxQRL99YOFhxvaRday+mOCqtUybDUvWEMXkVhLFCb8+cCLXEKIyPMMR8mCIh/eYkAZsYgRanRA2DAJ0fVFCfD6qJVq5xaCjEx5q3OrkIIliwsy5etPEYMBgqxKut1FtQKS4mHVckRZCLD95c7XvCAboHXpQuwT8UWTlUw7jg+AoL8e4uQrofCIX1tgZ/vtyb5Xu9Y8rAxURaPf9bEV8Bqrq8B60OU8Dbo7xW9/oQE4TvJbuDSFSgPk0EtOj8EjP4yRQO94M9k5mp/i2olYWWCjpFQ5E2p9ESoDx2Ty3aoLCuIUsD+M7vw3U3sprzC+ogC8jFVgI995LvIqJkj3Bx7I+T6z8Y8Ihg/FO3SbIEKO9tFX7kAheCwAxefYjG+VA7lsWICZgEN+xE= austi@Austin-Desktop - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDp2pYRPzxTAwXC0lplkhsJE02X/zlN9iVdrqejBEXbBSK9kUlXu2zsq0J+OGhCTsCoBDjYCE4j0UHQCnMyPFKX2uH1ZVV6jqeLjYvHAthEHoA1T5r/rByhKP8o8S3ud5TIOtD5en9LB8fBAd72865LrLr9v/9RybtKOErcf4Easb3GFLdjkrix0la1rduaBv6ZSCosM833FVVcr/GFwwroDURjd/yW9ROYbQWxfbFpA8B0srY1A6et9McvMoMTqfq5qv6NpPH+1QqAtAa2SjphbIpnVq0y8RL8k+f6frAcl2+qHjkAeURRC+r7yvNJmixyK/C4jJmjG2WtQ/fdHP+x1Kt/WDlGHEenpPR2IqIt5wMAbFES1MPRffHMgwY+c6N5Ia6yFZK/t9lrXglqwQcy4y97S035zw5TnxbRKowcnD39wX4ynfgeLDeHvrp6ZzjgLUM4UK7WvyiJ+EeOm4spy9OUnGYj0Bz2M3oIGEHzsCYJqfVoqDuxevQV69wdYXE= austi@Austin-Desktop sudo: [\u0026#39;ALL=(ALL) NOPASSWD:ALL\u0026#39;] groups: sudo shell: /bin/bash runcmd: - sudo su - - apt update \u0026amp;\u0026amp; apt -y install ca-certificates wget net-tools gnupg - wget -qO - https://as-repository.openvpn.net/as-repo-public.gpg | apt-key add - - echo \u0026#34;deb http://as-repository.openvpn.net/as/debian bionic main\u0026#34;\u0026gt;/etc/apt/sources.list.d/openvpn-as-repo.list - apt update \u0026amp;\u0026amp; apt -y install openvpn-as - cat /usr/local/openvpn_as/init.log \u0026gt; openvpnlog.txt Once these are setup on a machine with Terraform, and the Azure CLI is configured correctly, run:\n1 2 terraform init --upgrade terraform apply --auto-approve Retrieve the public\nSetup Cloud VM OpenVPN Server SSH into your cloud server using the public IP address.\nFollow this guide for setting up OpenVPN server Configure User Settings Create a user under User Management -\u0026gt; User Profiles Under User Management -\u0026gt; User Permissions, give the user Allow Auto-login rights. Under that user, hit More settings. Change Access Control to Use Routing. Then enter the network you wish the user to be able to access while on the VPN. Enable Access from both VPN clients and Server-side private subnets Change VPN Gateway to \u0026lsquo;Yes\u0026rsquo;, and enter client side subnets you wish to access. Configure VPN Settings Under the Configuration -\u0026gt; VPN Settings tab. Change Routing to \u0026lsquo;Yes, using Routing. Enter private sub nets clients should be able to access. Change \u0026lsquo;Allow access form these private subnets to all VPN client IP addresses and subnets\u0026rsquo; to \u0026lsquo;Yes\u0026rsquo; Change \u0026lsquo;Should clients be allowed to access network services on the VPN gateway IP address?\u0026rsquo; to \u0026lsquo;Yes\u0026rsquo; I recomeend enabling all traffic being sent over VPN, due to my slow connection at home I have it off. DNS settings are preference. Setup Home OpenVPN client For reference, this will be based on using an Ubuntu 20.04 VM.\nFirst install the client apt update -y \u0026amp;\u0026amp; apt install openvpn -y Download thee client.vpn file from your VPN server Navigate to the URL and sign in as the VPN user going to authenticate the client Upload the config file generated from the autologin profile of VPN user to the VM SCP to move file to VM ex: scp \\local\\file\\location\\client.ovpn user@server:\\etc\\openvpn Connect client to server by running openvpn --config client.ovpn --daemon Setup Client to forward IPv4 traffic We must enable IP forwarding along with NAT on the Ubuntu OpenVPN client in order for traffic to reach your internal services on the other end of the site-to-site VPN.\nThis is achieved with the following steps:\nEnable IP forwarding Edit /etc/sysctl.conf file nano /etc/sysctl.conf Uncomment the line net.ipv4.ip_forward=1 Restart sysctl to enable forwarding sysctl -p Enable NAT iptables -t nat -A POSTROUTING -j MASQUERADE Keep settings persistent between boots apt install iptables-persistent iptables-save \u0026gt; /etc/iptables/rules.v4 Automate connection at boot up By altering the ovpn into a config file, and moving it into the openvpn directory /etc/openvpn, at bootup the client will autheitnciate with the server if it is able to. Move client.ovpn file and rename it to client.conf\nmv /etc/openvpn/client.ovpn /etc/openvpn/client.conf Useful Resources OpenVPN Hugo Themes ","date":"2022-11-10T00:00:00Z","image":"https://cinderblock.github.io/p/openvpn-site-to-site-homelab/openvpn_sitetosite_huefec28f890f3a52601c6d36e2c7b3a5e_555051_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/openvpn-site-to-site-homelab/","title":"OpenVPN Site-to-Site - HomeLab"},{"content":"Overview Using Github pages, Github actions, and Hugo to create a static website for free.\nCheck out all of the configuration files on GitHub at the repository.\nPrerequisites Have a Domain name purchased (Will be demonstrating CloudFlare in this guide) Have a GitHub Account Steps Create a Public Repository Change Repository settings Custom Domain Name (Optional) Setup Hugo Picking a Theme Commit Create a Public Repository Go ahead and create a new repository. It is important that it is named something like example.github.io\nNext, navigate into the .github/workflows/ folder created by default. Create a file named example.yml (This should be the name as the example.github.io)\nAdd the following to the example.yml file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 name: github pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Node uses: actions/setup-node@v2 with: node-version: \u0026#39;14\u0026#39; - name: Install dependencies run: | npm install postcss -D npm install -g postcss-cli npm install -g autoprefixer - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; # extended: true - name: Build run: npm i \u0026amp;\u0026amp; hugo -D --gc #hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 #if: github.ref == \u0026#39;refs/heads/master\u0026#39; with: github_token: ${{ secrets.SECRET_HUGO }} publish_dir: ./public This will setup the GitHub page to build an environment that will support Hugo rather than the default GitHub pages service used Jekyll. The lines that run npm commands are additional, although necessary for some themes in order to run specific cross site scripts.\nThe github_token: ${{ secrets.SECRET_HUGO }} will be set under the Secrets -\u0026gt; Actions tab\nChange Repository Settings Go to the settings in the repository\nCreate a new branch - I use gh-pages Under Pages tab Under Build and Deployment -\u0026gt; Change source to Deploy from a branch and Branch to gh-pages (Optional) Change custom domain name to your purchased domain name Custom Domain Name (Optional) Setting up DNS for Domain to allow GitHub Pages If using CloudFlare, login to your dashboard and select the Domain Name to use. Go to DNS Create an A record for each of the following IP address pointing back at your example.com domain: 1 2 3 4 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Do the same but for IPv6 by Creating AAAA records for the following:\n1 2 3 4 2606:50c0:8000::153 2606:50c0:8001::153 2606:50c0:8002::153 2606:50c0:8003::153 Modify config.toml file (Generated later with Hugo Setup) Once you create a Hugo server, it will generate a plethora of files. One of these is a config file. Edit it and change the baseurl varaible to https://example.com/ Create Static site name file Again, once the Hugo site is created. There is a static folder. Navigate into it, create a file named CNAME. Add your Domain name to it and save it. Setup Hugo I recommend to clone down the repository created earlier, and generate the Hugo site into that folder structure. This will make commiting changes easier, and much more straightforward for this section.\nSetting up Hugo (There are a few extra steps compared to normal, due to the Theme I will be demonstrating requiring POSTCSS)\nThis example will cover conducting the setup in Linux\nCommands to Setup npm versioning (Should be V.14) - Necessary for PostCSS\n1 2 3 4 sudo apt update sudo apt install nodejs npm curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt install nodejs Setup Hugo\n1 2 wget https://github.com/gohugoio/hugo/releases/download/v0.98.0/hugo_extended_0.98.0_Linux-64bit.deb dpkg --install hugo_extended_0.98.0_Linux-64bit.deb Build site using Blist\n1 2 3 4 5 6 7 8 9 hugo new site blist # Can rename blist as needed cd blist git clone https://github.com/apvarun/blist-hugo-theme.git themes/blist cp themes/blist/package.json ./package.json cp themes/blist/package-lock.json ./package-lock.json npm install npm i -g postcss-cli # Necessary for PostCSS echo \u0026#34;theme = \\\u0026#34;blist\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; config.toml # Rename blist if you did so earlier hugo serve # Starts the Hugo server based on Static information within directory ran Picking a Theme If you wish to use another theme, I recommend searching online for freely available themes. Generally there isn\u0026rsquo;t too much change for setup in variation between theme to theme.\nCommit Now everytime you commit into the repository, it should update GH-Pages and populate the site!\nUseful Resources Hugo Hugo Themes ","date":"2022-10-08T00:00:00Z","image":"https://cinderblock.github.io/p/hugo-gitpage-creating-a-free-website/FreeHugoSiteGitPages_hub4f645c2bd0dad39cb67a0656939c5ed_913322_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/hugo-gitpage-creating-a-free-website/","title":"Hugo GitPage - Creating a free website"},{"content":"Overview Using Packer to create a Ubunut 22.04 server image within Proxmox. This is designed with Proxmox Virtual Environment version 7.1 in mind.\nCheck out all of the configuration files on GitHub at the repository.\nPrerequisites Must have Packer configured on your machine, DHCP running on the network, and a Proxmox server available (Preferrable to be version 7.1) Have a Proxmox user created with proper privledges for Terraform (See how to create the user here) Ubuntu Server 22.04 Iso uploaded to your Proxmox server Getting proper files setup There are two primary files that will need configured.\nThe first of the two is our credentials.pkr.hcl file. Update all variables within to your own. Any more in-depth configuration can be done within the ubuntu-server-focal.pkr.hcl file.\nYou can generate your token-id and token secret using the following command into your Proxmox Shell pveum user token add terraform-prov@pve terraform-token --privsep=0 Replace terraform-prov@pve with your created username Write this down because you won\u0026rsquo;t be able to find this access token again later\nAn Example credneitals/pkr.hcl file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 pm_api_url = \u0026#34;https://yourProxmox.server:8006/api2/json\u0026#34; pm_api_token_id = \u0026#34;full-tokenid\u0026#34; pm_api_token_secret = \u0026#34;tokenValue\u0026#34; pm_node = \u0026#34;nodeToBuildOn\u0026#34; pm_storage_pool = \u0026#34;storagePoolToBuildOn\u0026#34; pm_storage_pool_type = \u0026#34;typeOf-pm_storage_pool\u0026#34; ssh_username = \u0026#34;yourSshUser\u0026#34; ssh_password = \u0026#34;yourSshPassword\u0026#34; ssh_private_key_file = \u0026#34;~/.ssh/id_rsa\u0026#34; vm_name = \u0026#34;vm name\u0026#34; vm_id = \u0026#34;1003\u0026#34; iso_file = \u0026#34;local:iso/ubuntu-22.04-live-server-amd64.iso\u0026#34; #Iso file location on your proxmox Second file to update to your preference is our user-data.example file. You only need to update the fields beneath user-data, users + passwd, and potentially your timezone.\nThe passwd fields in cloud-init files must be in a sha-512 hash. Generate a hash using echo passwd | mkpasswd -m sha-512 -s An Example user-data file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #cloud-config autoinstall: version: 1 locale: en_US keyboard: layout: en ssh: install-server: true allow-pw: true disable_root: true ssh_quiet_keygen: true allow_public_ssh_keys: true packages: - qemu-guest-agent - sudo storage: layout: name: direct swap: size: 0 user-data: package_upgrade: false timezone: America/New_York users: - name: username-here groups: [adm, sudo] lock-passwd: false sudo: ALL=(ALL) NOPASSWD:ALL shell: /bin/bash # passwd: your-password # - or - # ssh_authorized_keys: # - your-ssh-key Finally, the Packer Process First thing we should do prior to going further, is double check the valiation of our Packer files. Otherwise we could have some major headache.\ncd into the project directory (./ubuntu-server-focal), and run packer validate -var-file='..\\credentials.pkr.hcl' .\\ubuntu-server-focal.pkr.hcl\nIf any errors show up, you\u0026rsquo;ll have to fix them before moving on Once we have confirmed everything appears correct, we can run the build, cross our fingers, and it should work.\nBuild the image (Within the same project directory where you ran the validate), packer build -var-file='..\\credentials.pkr.hcl' .\\ubuntu-server-focal.pkr.hcl Useful Resources Terraform Provider for Proxmox Proxmmox User Creation Guide Packer Tutoritals ","date":"2022-07-30T00:00:00Z","image":"https://cinderblock.github.io/p/packer-proxmox-ubuntu-server-creation/ProxmoxPacker_hu69eb84fdfe67bf1af478e5944900c509_469801_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/packer-proxmox-ubuntu-server-creation/","title":"Packer - Proxmox Ubuntu Server Creation"},{"content":"Overview Deplying helm charts in Kubernetes within Azure using the AKS service.\nBuild a cluster that is running a few services Have cluster automatically scale with load Have Kubeconfig file available so it can be managed, changed, altered, destroyed, etc. Ensure Kubeconfig file is secure, and is being encrypted with traffic involved in this Create a NGINX certificate service utilizing Cloudflare\u0026rsquo;s DNS Use Traefik as a loadbalancer, and utilize ingresses for reachable internal services Prerequisites Have an Azure account if you are a student, sign up for a student account and get some free credits along side it. Have a Cloudflare Account \u0026amp; a Domain Outlined in this post Create a public and private key Setup Cloudflare Obtain a public domain Generate a Token for DNS Read and write access Setup Terraform files for the deployment Providers Infrastructure Kubernetes, Kubectl, \u0026amp; Helm Assigning Variables Creating Output Run it Terraform State and Kubeconfig file Now What? Useful Resources Creating the Public \u0026amp; Private keys First step we\u0026rsquo;ll tackle is the creation of your public and private keys.\nI find the easiest way to do this, is to open up a terminal (Powershell and/or pretty much any linux terminal) and type in a quick few commands. To keep it short and sweet, we will just use ssh-keygen\nEnsure you save this to a locaiton you will remember, it\u0026rsquo;ll be important not to loose either key. This\u0026rsquo;ll allow you SSH access into your Kubernetes cluster.\nCopy your public key you genereated into the same folder as your .tf files will be located.\nSetting up Cloudflare Second step on our list, is to setup Cloudflare.\nCloudflare is important here, since it will be handing out certificates to services running within the Kubernetes cluster. Luckily, signing up for Cloudflare is free, and I\u0026rsquo;ll go over how to obtain a Token for API access.\nOnce you have created your account, and have obtained a public domain, you should see a page similar to the following;\nScroll down on the overview tab, and on the right hand side, there should be a \u0026lsquo;Get your API token\u0026rsquo; link. On the following page, click \u0026lsquo;Create token\u0026rsquo;\nScroll down to the bottom, and select \u0026lsquo;Create Custom Token`. On the following page, ensure you give your token a memorable name, assign it permissions to read and edit DNS zone settings, and limit it to your respective Zone resources (Domain). Set the TTL to the duration of your project.\nContinue to summary, and collect the API token key, store it discretely. This Token will be used in Terraform, within the .tfvars file later for authentication with the Cloudflare API.\nTerraform Process I prefer to seperate my Terraform files for readability, feel free to see all the code at a glance on the Github Repo. I\u0026rsquo;ll do a breakdown here in this section.\nSetting up Providers; Azurerm, Kubernetes, Helm, Kubectl In this situation, I am authenticating to Azure using their AzureCLI. Refer to Microsoft\u0026rsquo;s official documentation for Azure CLI setup. Kubernetes, Kubectl, and Helm don\u0026rsquo;t require and specific authentication. The API Token generated earlier to authenticate with Cloudflare. This\u0026rsquo;ll be defined in the .tfvars file.\nTake the following code, and put it into a file called providers.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 terraform { required_version = \u0026#34;\u0026gt;= 0.14.8\u0026#34; required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 3.0.2\u0026#34; } helm = { source = \u0026#34;hashicorp/helm\u0026#34; version = \u0026#34;~\u0026gt;2.5.1\u0026#34; } kubernetes = { source = \u0026#34;hashicorp/kubernetes\u0026#34; version = \u0026#34;~\u0026gt; 2.8.0\u0026#34; } kubectl = { source = \u0026#34;gavinbunney/kubectl\u0026#34; version = \u0026#34;~\u0026gt; 1.14.0\u0026#34; } cloudflare = { source = \u0026#34;cloudflare/cloudflare\u0026#34; version = \u0026#34;~\u0026gt; 3.16.0\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} } provider \u0026#34;kubernetes\u0026#34; { host = data.azurerm_kubernetes_cluster.credneitals.kube_config.0.host client_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_certificate) client_key = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_key) cluster_ca_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.cluster_ca_certificate) } provider \u0026#34;helm\u0026#34; { kubernetes { host = data.azurerm_kubernetes_cluster.credneitals.kube_config.0.host client_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_certificate) client_key = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_key) cluster_ca_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.cluster_ca_certificate) } } provider \u0026#34;kubectl\u0026#34; { host = data.azurerm_kubernetes_cluster.credneitals.kube_config.0.host client_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_certificate) client_key = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_key) cluster_ca_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.cluster_ca_certificate) load_config_file = false } provider \u0026#34;cloudflare\u0026#34; { # Comment out key \u0026amp; email if using token #email = var.cloudflare_email #api_key = var.cloudflare_api_key api_token = var.cloudflare_token } Setting up infrastructure I sepereated the \u0026lsquo;structure\u0026rsquo; into a few seperate files. One for networking aspects in Azure; networking.tf. Another for the k8s cluster itself (AKS); cluster.tf.\nWithin the networking file, it will setup firewall rules for the virtual private network, create the network itself, and create a load balancer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 # Resource Group for Terraform deployment resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-cluster\u0026#34; location = var.region } # Networking Setup for Cluster resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-network\u0026#34; location = azurerm_resource_group.cluster.location resource_group_name = azurerm_resource_group.cluster.name address_space = [\u0026#34;10.1.0.0/16\u0026#34;] } # Assign subnet for Cluster resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-subnet\u0026#34; virtual_network_name = azurerm_virtual_network.cluster.name resource_group_name = azurerm_resource_group.cluster.name address_prefixes = [\u0026#34;10.1.0.0/24\u0026#34;] } # Firewall settings for cluster resource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;Allow-RDP-SSH-KCTL\u0026#34; location = azurerm_resource_group.cluster.location resource_group_name = azurerm_resource_group.cluster.name security_rule { name = \u0026#34;HTTP\u0026#34; priority = 101 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;80\u0026#34; destination_port_range = \u0026#34;80\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { name = \u0026#34;HTTPS\u0026#34; priority = 102 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;443\u0026#34; destination_port_range = \u0026#34;443\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { name = \u0026#34;k8s-api\u0026#34; priority = 103 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;6443\u0026#34; destination_port_range = \u0026#34;6443\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } } # Force Terraform to wait resource \u0026#34;time_sleep\u0026#34; \u0026#34;wait_for_kubernetes\u0026#34; { depends_on = [azurerm_kubernetes_cluster.cluster] create_duration = \u0026#34;20s\u0026#34; } # Create public IP for load balancer resource \u0026#34;azurerm_public_ip\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;K8S-PublicIPAddress\u0026#34; resource_group_name = azurerm_resource_group.cluster.name location = azurerm_resource_group.cluster.location allocation_method = \u0026#34;Static\u0026#34; } # Assign loadbalancer to a Data variable, for user later resource \u0026#34;azurerm_lb\u0026#34; \u0026#34;traefik_lb\u0026#34; { depends_on = [helm_release.traefik] name = \u0026#34;k8s-traefik-lb\u0026#34; resource_group_name = azurerm_resource_group.cluster.name location = azurerm_resource_group.cluster.location frontend_ip_configuration { name = azurerm_public_ip.cluster.name public_ip_address_id = azurerm_public_ip.cluster.id } } Within the cluster file, we\u0026rsquo;ll define the virtual machine AKS structure being created.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Create Kubernetes Cluster resource \u0026#34;azurerm_kubernetes_cluster\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-aks\u0026#34; location = azurerm_resource_group.cluster.location resource_group_name = azurerm_resource_group.cluster.name dns_prefix = \u0026#34;${var.prefix}-aks\u0026#34; linux_profile { admin_username = var.linux_user ssh_key { key_data = file(var.ssh_public_key) } } default_node_pool { name = \u0026#34;agentpool\u0026#34; node_count = var.cluster_nodes_count vm_size = \u0026#34;Standard_B2s\u0026#34; type = \u0026#34;VirtualMachineScaleSets\u0026#34; #availability_zones = [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;] enable_auto_scaling = true min_count = 1 max_count = 3 # Required for advanced networking vnet_subnet_id = azurerm_subnet.cluster.id } ### Uncomment this and add id/secret for management #service_principal { # client_id = var.aks_service_principal_app_id # client_secret = var.aks_service_principal_client_secret #} identity { type = \u0026#34;SystemAssigned\u0026#34; } network_profile { network_plugin = \u0026#34;kubenet\u0026#34; load_balancer_sku = \u0026#34;standard\u0026#34; } #addon_profile { # oms_agent { # enabled = true # log_analytics_workspace_id = azurerm_log_analytics_workspace.test.id # } #} tags = { Environment = \u0026#34;Development\u0026#34; } } # Assign credentials to data, used for helm, kubernetes, and kubectl providers. data \u0026#34;azurerm_kubernetes_cluster\u0026#34; \u0026#34;credneitals\u0026#34; { name = azurerm_kubernetes_cluster.cluster.name resource_group_name = azurerm_resource_group.cluster.name depends_on = [azurerm_kubernetes_cluster.cluster] } # Pull kubeconfig to your local machine resource \u0026#34;local_file\u0026#34; \u0026#34;kubeconfig\u0026#34; { depends_on = [azurerm_kubernetes_cluster.cluster] filename = \u0026#34;./kubeconfig\u0026#34; content = azurerm_kubernetes_cluster.cluster.kube_config_raw } Setting up the Kubernetes \u0026amp; Helm attributes First file we\u0026rsquo;ll make is a nginx.tf file. Within it we will define the manifest attributes of the deployment, and setup an ingress for accessing the service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 # nginx deployment resource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;nginx\u0026#34; { depends_on = [ azurerm_kubernetes_cluster.cluster ] metadata { name = \u0026#34;nginx\u0026#34; } } # Create the YAML configuration for nginx within the kubernetes provider resource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;nginx\u0026#34; { depends_on = [ kubernetes_namespace.nginx ] metadata { name = \u0026#34;nginx\u0026#34; namespace = \u0026#34;nginx\u0026#34; labels = { app = \u0026#34;nginx\u0026#34; } } spec { replicas = 1 selector { match_labels = { app = \u0026#34;nginx\u0026#34; } } template { metadata { labels = { app = \u0026#34;nginx\u0026#34; } } spec { container { image = \u0026#34;nginx:latest\u0026#34; name = \u0026#34;nginx\u0026#34; port { container_port = 80 } } } } } } # Set namespace and port assignments for nginx access resource \u0026#34;kubernetes_service\u0026#34; \u0026#34;nginx\u0026#34; { depends_on = [kubernetes_namespace.nginx] metadata { name = \u0026#34;nginx\u0026#34; namespace = \u0026#34;nginx\u0026#34; } spec { selector = { app = \u0026#34;nginx\u0026#34; } port { port = 80 } type = \u0026#34;ClusterIP\u0026#34; } } # Create ingress for NGINX - Allow outside communication to it resource \u0026#34;kubernetes_ingress_v1\u0026#34; \u0026#34;nginx\u0026#34; { depends_on = [kubernetes_namespace.nginx] metadata { name = \u0026#34;nginx\u0026#34; namespace = \u0026#34;nginx\u0026#34; } spec { rule { host = \u0026#34;${var.cloudflare_domainname}\u0026#34; http { path { path = \u0026#34;/\u0026#34; backend { service { name = \u0026#34;nginx\u0026#34; port { number = 80 } } } } } } tls { secret_name = \u0026#34;nginx\u0026#34; hosts = [\u0026#34;${var.cloudflare_domainname}\u0026#34;] } } } # Assign deployments/nginx-cert.yml file to a data value data \u0026#34;kubectl_path_documents\u0026#34; \u0026#34;nginx\u0026#34; { pattern = \u0026#34;./deployments/nginx-cert.yml\u0026#34; vars = { cloudflare-domainname = \u0026#34;${var.cloudflare_domainname}\u0026#34; } } # Set nginx config, pulls yaml information from deployments/nginx-cert.yml resource \u0026#34;kubectl_manifest\u0026#34; \u0026#34;nginx-certificate\u0026#34; { for_each = toset(data.kubectl_path_documents.nginx.documents) yaml_body = each.value depends_on = [kubernetes_namespace.nginx, time_sleep.wait_for_clusterissuer] } The second step setup, is to define cloudflare.tf file for setting up Cloudflare and the certmanager aspect. This will be done by using Kubectl manifest, namespace creation, and a Helm chart deployment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 # Assign namespace for certmanager resource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;certmanager\u0026#34; { depends_on = [ azurerm_kubernetes_cluster.cluster ] metadata { name = \u0026#34;certmanager\u0026#34; } } # Intiates Cloudflare secret for Kubernetes resource \u0026#34;kubernetes_secret\u0026#34; \u0026#34;cloudflare_api_key_secret\u0026#34; { depends_on = [ kubernetes_namespace.certmanager ] metadata { name = \u0026#34;cloudflare-api-key-secret\u0026#34; namespace = \u0026#34;certmanager\u0026#34; } data = { api-key = \u0026#34;${var.cloudflare_api_key}\u0026#34; } type = \u0026#34;Opaque\u0026#34; } # Assign deployments/cloudflare.yml file to a data value data \u0026#34;kubectl_path_documents\u0026#34; \u0026#34;cloudflare\u0026#34; { pattern = \u0026#34;./deployments/cloudflare.yml\u0026#34; vars = { cloudflare-email = \u0026#34;${var.cloudflare_email}\u0026#34; } } # Create a ClusterIssuer, pulls yaml information from deployments/cloudflare.yml resource \u0026#34;kubectl_manifest\u0026#34; \u0026#34;cloudflare_prod\u0026#34; { for_each = toset(data.kubectl_path_documents.cloudflare.documents) yaml_body = each.value depends_on = [time_sleep.wait_for_certmanager] } resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;cluster\u0026#34; { zone_id = var.cloudflare_zonid name = var.cloudflare_domainname value = azurerm_public_ip.cluster.ip_address type = \u0026#34;A\u0026#34; proxied = false depends_on = [azurerm_lb.traefik_lb] } # Force Terraform to wait resource \u0026#34;time_sleep\u0026#34; \u0026#34;wait_for_clusterissuer\u0026#34; { depends_on = [ kubectl_manifest.cloudflare_prod ] create_duration = \u0026#34;30s\u0026#34; } # Use helm to deploy certmanager in cluster resource \u0026#34;helm_release\u0026#34; \u0026#34;certmanager\u0026#34; { depends_on = [ kubernetes_namespace.certmanager ] name = \u0026#34;certmanager\u0026#34; namespace = \u0026#34;certmanager\u0026#34; repository = \u0026#34;https://charts.jetstack.io\u0026#34; chart = \u0026#34;cert-manager\u0026#34; set { name = \u0026#34;installCRDs\u0026#34; value = \u0026#34;true\u0026#34; } } # Force Terraform to wait resource \u0026#34;time_sleep\u0026#34; \u0026#34;wait_for_certmanager\u0026#34; { depends_on = [ helm_release.certmanager ] create_duration = \u0026#34;10s\u0026#34; } Third we will create the traefik.tf file. This will allow use to utilize loadbalacning and ingress.Traefik will be implemented using Helm.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # Traefik Deployment resource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;traefik\u0026#34; { metadata { name = \u0026#34;traefik\u0026#34; } } resource \u0026#34;helm_release\u0026#34; \u0026#34;traefik\u0026#34; { depends_on = [ kubernetes_namespace.traefik ] name = \u0026#34;traefik\u0026#34; namespace = \u0026#34;traefik\u0026#34; repository = \u0026#34;https://helm.traefik.io/traefik\u0026#34; chart = \u0026#34;traefik\u0026#34; set { name = \u0026#34;ingressClass.enabled\u0026#34; value = \u0026#34;true\u0026#34; } set { name = \u0026#34;ingressClass.isDefaultClass\u0026#34; value = \u0026#34;true\u0026#34; } set { name = \u0026#34;ports.web.redirectTo\u0026#34; value = \u0026#34;websecure\u0026#34; } set { name = \u0026#34;ports.websecure.tls.enabled\u0026#34; value = \u0026#34;true\u0026#34; } } data \u0026#34;kubernetes_service\u0026#34; \u0026#34;traefik\u0026#34; { metadata { name = helm_release.traefik.name namespace = helm_release.traefik.namespace } } Assigning variable to variables.tfvars First create and define variables within a variables.tf file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 variable \u0026#34;cluster_name\u0026#34; { default = \u0026#34;terraformclust\u0026#34; } variable \u0026#34;cluster_nodes_count\u0026#34; { default = \u0026#34;2\u0026#34; } variable \u0026#34;region\u0026#34; { default = \u0026#34;East US 2\u0026#34; } variable \u0026#34;prefix\u0026#34; { default = \u0026#34;test\u0026#34; } variable \u0026#34;ssh_public_key\u0026#34;{ default = \u0026#34;./id_rsa.pub\u0026#34; } variable \u0026#34;cloudflare_api_key\u0026#34; {} variable \u0026#34;cloudflare_email\u0026#34; {} variable \u0026#34;cloudflare_api_key_secret\u0026#34; {} variable \u0026#34;cloudflare_prod_account_key\u0026#34; {} variable \u0026#34;cloudflare_zonid\u0026#34; {} variable \u0026#34;cloudflare_domainname\u0026#34; {} variable \u0026#34;cloudflare_token\u0026#34; {} variable \u0026#34;linux_user\u0026#34; {} Assign correct Cloudflare information into the tfvariables.auto.tfvars file. Follow along with the tfvariables.auto.tfvars.example file.\n1 2 3 4 5 6 7 8 9 cloudflare_api_key = \u0026#34;api key here\u0026#34; # This should have DNS read/write permissions in Cloudflare cloudflare_email = \u0026#34;email associated to account\u0026#34; # Used to login with cloudflare_api_key_secret = \u0026#34;Cloudflare key secret\u0026#34; # Used for cloudflare.yml file cloudflare_prod_account_key = \u0026#34;Cloudflare production account key\u0026#34; # Used for cloudflare.yml file cloudflare_zonid = \u0026#34;Zone ID for cloudflare account\u0026#34; # Used for Cloudflare resource cloudflare_domainname = \u0026#34;domain name\u0026#34; # Used for Cloudflare resource cloudflare_token = \u0026#34;cloudflare token\u0026#34; linux_user = \u0026#34;username\u0026#34; #Admin access to cluster master(s) As for the SSH key, ensure you have a .pub file that you are pulling data from, or directly put SSH key into into variable file.\nCreating output to be sent back after Terraform finishes running An important step, is creating necessary output data to result from running the terraform apply. In this scenario, it is critical to have data from the cluster such as certificate information, the kubeconfig, and cluster credentials. Luckily, since these are obviously sensitive data, we can force a sensitive = true attribute to them, and just have the State file hold onto that information. This\u0026rsquo;ll allow us to pull the Kube config to control the K8S cluster to our machine.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # It is necessary to identify these variables for usage later when updating the state file or using kubectl commands output \u0026#34;client_key\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.client_key sensitive = true } output \u0026#34;client_certificate\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.client_certificate sensitive = true } output \u0026#34;cluster_ca_certificate\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.cluster_ca_certificate sensitive = true } output \u0026#34;cluster_username\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.username sensitive = true } output \u0026#34;cluster_password\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.password sensitive = true } output \u0026#34;kube_config\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config_raw sensitive = true } output \u0026#34;host\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.host sensitive = true } # Critical to get kubectl file connected out to Azure for local environment output \u0026#34;cluster_name\u0026#34; { value = azurerm_kubernetes_cluster.cluster.name } output \u0026#34;resource_group_name\u0026#34; { value = azurerm_resource_group.cluster.name } #Public IP of Cluster output \u0026#34;cluster_public_ip\u0026#34; { value = azurerm_public_ip.cluster.ip_address } Run it Once everything is setup and ready to roll, navigate into the directory containing all terraform files.\nRun a terraform init Then terraform validate If everything checks out, run terraform apply, and in roughly 5 minutes you should have a running AKS cluster in Azure! Gain access to Kubectl In order to gain access from your local machine, we will use the azure CLI. If you followed the tutorial, you\u0026rsquo;ll already be logged in, az login. Use the following command to set your environment varialbe for kubectl to control kubernetes cluster in Azure. az aks get-credentials --resource-group $(terraform output -raw resource_group_name) --name $(terraform output -raw cluster_name)\nOnce ran, you can verify it is connected and working with kubectl get nodes , kubectl get namespace\nNow what? Congragulations! The hard part of getting started, is now over. You now have a ready to spin up AKS cluster in Azure prebuilt with a loadbalancer and certificate automation. From here, the possibilites are limitless.\nUseful Resources Kubernetes Overview Terraform Kubernetes Terraform Azurerm Terraform Helm https://github.com/xcad2k/boilerplates/tree/main/terraform/templates/kubernetes-automation-example https://www.youtube.com/watch?v=kFt0OGd_LhI\u0026t=870s\n","date":"2022-06-16T00:00:00Z","image":"https://cinderblock.github.io/p/terraform-azure-kubernetes-with-helm-charts-and-cloudflare/AzureK8SHelm_hu8e324d8d48179918ac16116902203964_216689_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/terraform-azure-kubernetes-with-helm-charts-and-cloudflare/","title":"Terraform - Azure Kubernetes with Helm Charts and Cloudflare"},{"content":"Overview Manipulating spotify playlist using Terraform, because why not?\nCheck out all of the configuration files on GitHub at the repository. I have various examples of Terraform using the Spotify Provider within as well!\nUnderstanding the provider For all Spotify API usage with Terraform, You must either run a oauth server locally and just provide the API key, or use something online and provide all 4 necessary features (api, token_id, user, and oauth_url). You will need to create a Spotify Developer account here If you are unfamiliar with running an oauth server, I highly recommend to use this oauth server provided by the creator of the Terraform Spotify Provider! Check it out on his Github page. All the necessary instructions to get started with it will be on there.\nMulti-Artist Playlist Creation The following code will represent how to use Terraform to create a Multi-artist playlist based on artist names\nCreating the Provider.tf file In here, I defined the version to use for the provider, along with setting variables for auth_server, api_key, username, and token_id\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 terraform { required_providers { spotify = { source = \u0026#34;conradludgate/spotify\u0026#34; version = \u0026#34;0.2.7\u0026#34; } } } provider \u0026#34;spotify\u0026#34; { # Refernced in tfvariables.tfvars auth_server = \u0026#34;${var.spotify_oauth_url}\u0026#34; api_key = \u0026#34;${var.spotify_api_key}\u0026#34; username = \u0026#34;${var.spotify_user}\u0026#34; token_id = \u0026#34;${var.spotify_token_id}\u0026#34; } Creating the playlist.tf file This will be where the core of the Terraform structure is provided. It will use variables defined in the other files to loop through and generate the desired playlist.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Create the playlist resource \u0026#34;spotify_playlist\u0026#34; \u0026#34;custom\u0026#34; { depends_on = [ data.spotify_search_track.custom_count, ] name = var.playlist_name description = var.playlist_desc public = true tracks = flatten([ local.all_track_ids ]) } # Find ids for songs data \u0026#34;spotify_search_track\u0026#34; \u0026#34;custom_count\u0026#34; { count = length(var.artist_list) artist = \u0026#34;${var.artist_list[count.index]}\u0026#34; limit = 10 explicit = true } # Local Variables to concat id lists into one locals { depends_on = [data.spotify_search_track.custom_count] all_tracks = concat(data.spotify_search_track.custom_count[*]) all_track_ids = flatten(local.all_tracks[*].tracks[*].id) all_track_names = flatten(local.all_tracks[*].tracks[*].name) } output \u0026#34;list\u0026#34; { value = local.all_track_names } Create your varialbes in a variables.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 variable \u0026#34;spotify_api_key\u0026#34; { type = string description = \u0026#34;Set this as the APIKey that the authorization proxy server outputs\u0026#34; } variable \u0026#34;spotify_token_id\u0026#34; { type = string description = \u0026#34;Token ID provided from oauth\u0026#34; } variable \u0026#34;spotify_user\u0026#34; { type = string description = \u0026#34;username for oauth\u0026#34; } variable \u0026#34;spotify_oauth_url\u0026#34; { type = string description = \u0026#34;url path of oauth server\u0026#34; } variable \u0026#34;playlist_name\u0026#34; { type = string description = \u0026#34;name of playlist\u0026#34; } variable \u0026#34;playlist_desc\u0026#34; { type = string description = \u0026#34;desc of playlist\u0026#34; } variable \u0026#34;artist_list\u0026#34; { type = list description = \u0026#34;Spotify Artists\u0026#34; } Define the variables.auto.tfvars file Ensure to change the information within this file to fit your needs In this file, reference your spotify_api_key, spotify_token_id, spotify_oauth_url, and spotify_user\n1 2 3 4 5 6 7 spotify_api_key = \u0026#34;Spotify API key here\u0026#34; spotify_user = \u0026#34;oauth User\u0026#34; spotify_token_id = \u0026#34;oauth Token Here\u0026#34; spotify_oauth_url = \u0026#34;oauth site here\u0026#34; playlist_name = \u0026#34;name\u0026#34; playlist_desc =\u0026#34;description\u0026#34; artist_list = [\u0026#34;Artist\u0026#34;, \u0026#34;Go\u0026#34;, \u0026#34;Here\u0026#34;] Creating the playlist After the aformentioned files are created and customized to your liking, just execute the terraform commands:\n1 2 terraform init terraform apply --auto-approve Open your spotify and see the newly genereated playlist, and delete the newly generated list with:\n1 terraform destroy --auto-approve Since Terraform is a stateful deployment, as long as you have control of the state file that gets created alongside Terraform, you can manage and alter the created playlist as you wish.\nUseful Resources Refer to this official spotify guide for creating a self-hosted oauth server, or using the free open source one the creater made: Github\nSpotify Provideron Terraform Provider list\n","date":"2022-05-02T00:00:00Z","image":"https://cinderblock.github.io/p/terraform-spotify-automation/SpotifyAutomation_hub0bd49f7622d39e4013fd6e22c6a0b5f_625830_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/terraform-spotify-automation/","title":"Terraform - Spotify Automation"},{"content":"Overview Convert existing resources in Azure to Terraform files (Terrafy/aztfy)\nMicrosoft Recently released a sweet tool called aztfy. This enables you to \u0026rsquo;terrafy\u0026rsquo; existing Azure resources. This effectively allows for total conversions of existing data in Azure to be modified and updated/managed using Terraform. Whether it be for backup reasons, future management of resource changes, or cloud implmentation purposes, this could prove extremely useful. Prerequisites Have Go installed in your dev environment For Windows, go to this site and install Go Go to Microsoft\u0026rsquo;s Azure Aztfy Github Page Pull the repository to your local machine and install the go module they provide for aztfy 1 2 3 git clone https://github.com/Azure/aztfy cd \u0026#34;Directory-You-Cloned-To\u0026#34; go install Using Aztfy Once you have a resource group in Azure, you can run the following command to pull specific resources from that group\naztfy Azure-Resource-Group-Here From here, you can navigate each individual resource within that group and import them individually from Azure to your local machine. It generates the .tf files, terraform state files, and all necessary JSON components.\nExample of Aztfy Result: Navigate into directory you cloned aztfy to, and double check the clone worked by attempting a terraform apply and if it says no changes needed, you are solid!\nUseful Resources Terraform Resources Microsoft Blog Post Azure Documentation for AzAPI ","date":"2022-04-27T00:00:00Z","image":"https://cinderblock.github.io/p/terrifying-or-converting-azure-resources-into-terraform-code/AzureTerrify_hud2d7185e8cbfb57e1a609141917a3c4f_215916_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/terrifying-or-converting-azure-resources-into-terraform-code/","title":"'Terrifying' or Converting Azure Resources into Terraform Code"},{"content":"Overview Using Terraform to deploy virtual machines in Proxmox. This is designed with Proxmox Virtual Environment version 7.1 in mind.\nCheck out all of the configuration files on GitHub (proxmox-deploy-vm) at the repository!\nPurpose I have a Proxmox server in my homelab, and wanted to have an easier way to spin up virtual machines on an as needed basis.\nPrequisites Have a Proxmox server Have a template made on Proxmox, in my case, I used Ubuntu Server 20.04. Have Terraform installed Steps Create service user in Proxmox with appropriate settings for Terraform deployments Configure Terraform files \u0026amp; variables for deployment Deploy Terraform 1. Configure/Create Terraform User \u0026amp; Role This can be also be done under Datacenter\u0026ndash;\u0026gt;\u0026ldquo;Proxmox server\u0026rdquo; \u0026ndash;\u0026gt; Permissions \u0026ndash;\u0026gt; Users \u0026amp; Roles in the GUI.\nCreate user and assign it a role SSH into the Proxmox server Create the role in Proxmox with appropriate permissions pveum role add TerraformProv -privs \u0026quot;VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Audit VM.Console\u0026quot; Create the user pveum user add terraform-prov@pve --password \u0026lt;password\u0026gt; Assign user to the role pveum aclmod / -user terraform-prov@pve -role TerraformProv OPTIONAL: Create a token tom use rather than username and password pveum user token add terraform-prov@pve terraform-token --privsep=0 2. Configure Terraform files This is broken down into 4 files. main.tf, linuxvm.tf, variables.tf, and variables.auto.tfvars.\nCreate main.tf file This file will host Terraform provider information and versioning, along with the connection parameters to your Proxmox server. Put the following into it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 terraform { required_providers { proxmox = { source = \u0026#34;Telmate/proxmox\u0026#34; version = \u0026#34;\u0026gt;=2.9.6\u0026#34; } } } provider \u0026#34;proxmox\u0026#34; { pm_api_url = var.PM_URL pm_user = var.PM_USER pm_password = var.PM_PASS } Create linuxvm.tf file Contains code to deploy the Virtual Machine.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Setup Cloud-init w/ https://pve.proxmox.com/wiki/Cloud-Init_Support /* Uses Cloud-Init options from Proxmox 5.2 */ resource \u0026#34;proxmox_vm_qemu\u0026#34; \u0026#34;cloudinit-test\u0026#34; { name = var.linux_fqdn desc = \u0026#34;terraform deploy\u0026#34; target_node = var.PM_node clone = var.PM_template # The destination resource pool for the new VM pool = \u0026#34;\u0026#34; storage = \u0026#34;datastore\u0026#34; cores = 1 sockets = 1 memory = 2048 disk_gb = 8 nic = \u0026#34;virtio\u0026#34; bridge = \u0026#34;vmbr0\u0026#34; ssh_user = \u0026#34;root\u0026#34; ssh_private_key = var.ssh_priv os_type = \u0026#34;cloud-init\u0026#34; ipconfig0 = \u0026#34;ip=${var.linux_ip}/${var.linux_subnetmask},gw=${var.linux_gateway}\u0026#34; sshkeys = var.ssh_keys provisioner \u0026#34;remote-exec\u0026#34; { inline = [ \u0026#34;ip a\u0026#34; ] } } Create variables.tf file Variables declared here, which are defined in the .tfvars file.\n1 2 3 4 5 6 7 8 9 10 11 12 variable \u0026#34;PM_USER\u0026#34; {} variable \u0026#34;PM_PASS\u0026#34; {} variable \u0026#34;PM_URL\u0026#34; {} variable \u0026#34;PM_node\u0026#34; {} variable \u0026#34;PM_template\u0026#34; {} variable \u0026#34;ssh_keys\u0026#34; {} variable \u0026#34;ssh_priv\u0026#34; {} variable \u0026#34;ssh_user\u0026#34; {} variable \u0026#34;linux_fqdn\u0026#34; {} variable \u0026#34;linux_ip\u0026#34; {} variable \u0026#34;linux_subnetmask\u0026#34; {} variable \u0026#34;linux_gateway\u0026#34; {} Create variables.auto.tfvars file Put your custom defined variables into this file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Example .tfvars file PM_PASS = \u0026#34;proxmox terraform user password\u0026#34; PM_USER = \u0026#34;proxmox terraform user username\u0026#34; PM_URL = \u0026#34;https://IPADDRESSorFQDN:8006/api2/json\u0026#34; PM_node = \u0026#34;target proxmox server name\u0026#34; PM_template = \u0026#34;target tempalte in Proxmox\u0026#34; linux_fqdn = \u0026#34;desired name.domain\u0026#34; linux_ip = \u0026#34;desired IP\u0026#34; linux_subnetmask = \u0026#34;subnet mask of network\u0026#34; # Should be 24, 26, 30, etc. linux_gateway = \u0026#34;gateway ip\u0026#34; ssh_user = \u0026#34;linux username\u0026#34; ssh_keys = \u0026lt;\u0026lt;EOF ssh-rsa key here ssh-rsa key here EOF ssh_priv = \u0026lt;\u0026lt;EOF -----BEGIN OPENSSH PRIVATE KEY----- private keye here -----END OPENSSH PRIVATE KEY----- EOF 3. Deploy terraform Navigate to directory, run terraform init terraform apply --auto-approve Check Proxmox server to see VM\nDestory it with terraform destroy --auto-approve Useful Resources Terraform Provider for Proxmox Proxmmox User Creation Guide ","date":"2022-04-14T00:00:00Z","image":"https://cinderblock.github.io/p/terraform-proxmox-virtual-machine-deploy/ProxmoxHot_hu4752146b461923c8ef4d5fe7eef88e61_439896_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/terraform-proxmox-virtual-machine-deploy/","title":"Terraform - Proxmox Virtual Machine Deploy"},{"content":"Overview Deplying a Kubernetes Cluster in Azure with the AKS service.\nBuild a Kubernetes cluster in Azure Have the cluster setup to automatically scale with load Have Kubeconfig file available so it can be managed, changed, altered, destroyed, etc. Ensure Kubeconfig file is secure, and is being encrypted with traffic involved in this Check out all of the configuration files on GitHub (Azure-K8S-Deploy) at the repository!\nSteps to do this Have an Azure account; if you are a student, sign up for a student account and get some free credits along side it. Setup Terraform files for the deployment Keep track of Terraform State and Kubeconfig files in order to continue managaing deployed resources Terraform Process Setting up Providers; Azurerm \u0026amp; Kubernetes Create a file named provider.tf I personally use the Azure CLI, this guide will be based on that. Use az login to connect to your Azure resources. Refer to Microsoft\u0026rsquo;s documentation for Azure CLI. For Terraform, we are using the Azurerm, and Kubernetes Poviders\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 terraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;3.0.2\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} } provider \u0026#34;kubernetes\u0026#34; { host = data.azurerm_kubernetes_cluster.credneitals.kube_config.0.host client_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_certificate) client_key = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.client_key) cluster_ca_certificate = base64decode(data.azurerm_kubernetes_cluster.credneitals.kube_config.0.cluster_ca_certificate) } Setting up the Kubernetes structure Create a file nameed cluster.tf. Within this file, we will define required networking and cluster resources/data parameters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 # Resource Group for Terraform deployment resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-cluster\u0026#34; location = var.region } # Networking Setup for Cluster resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-network\u0026#34; location = azurerm_resource_group.cluster.location resource_group_name = azurerm_resource_group.cluster.name address_space = [\u0026#34;10.1.0.0/16\u0026#34;] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-subnet\u0026#34; virtual_network_name = azurerm_virtual_network.cluster.name resource_group_name = azurerm_resource_group.cluster.name address_prefixes = [\u0026#34;10.1.0.0/24\u0026#34;] } resource \u0026#34;azurerm_kubernetes_cluster\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${var.prefix}-aks\u0026#34; location = azurerm_resource_group.cluster.location resource_group_name = azurerm_resource_group.cluster.name dns_prefix = \u0026#34;${var.prefix}-aks\u0026#34; linux_profile { admin_username = \u0026#34;ubuntu\u0026#34; ssh_key { key_data = file(var.ssh_public_key) } } default_node_pool { name = \u0026#34;agentpool\u0026#34; node_count = var.cluster_nodes_count vm_size = \u0026#34;Standard_B2s\u0026#34; type = \u0026#34;VirtualMachineScaleSets\u0026#34; #availability_zones = [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;] enable_auto_scaling = true min_count = 1 max_count = 3 # Required for advanced networking vnet_subnet_id = azurerm_subnet.cluster.id } identity { type = \u0026#34;SystemAssigned\u0026#34; } network_profile { network_plugin = \u0026#34;kubenet\u0026#34; load_balancer_sku = \u0026#34;standard\u0026#34; } tags = { Environment = \u0026#34;Development\u0026#34; } } # Required to be pushed into Kubernetes Provid`er in provider.tf data \u0026#34;azurerm_kubernetes_cluster\u0026#34; \u0026#34;credneitals\u0026#34; { name = azurerm_kubernetes_cluster.cluster.name resource_group_name = azurerm_resource_group.cluster.name depends_on = [azurerm_kubernetes_cluster.cluster] } # This will bring down the required kubeconfig locally to your machine resource \u0026#34;local_file\u0026#34; \u0026#34;kubeconfig\u0026#34; { depends_on = [azurerm_kubernetes_cluster.cluster] filename = \u0026#34;./kubeconfig\u0026#34; content = azurerm_kubernetes_cluster.cluster.kube_config_raw } Assigning variables for Terraform Create a file named variables.tf. Generally, these variables within the file are ready to go. Ensure the public key is pointing to your SSH key.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 variable \u0026#34;cluster_name\u0026#34; { default = \u0026#34;terraformclust\u0026#34; } variable \u0026#34;cluster_nodes_count\u0026#34; { default = \u0026#34;2\u0026#34; } variable \u0026#34;region\u0026#34; { default = \u0026#34;East US 2\u0026#34; } variable \u0026#34;prefix\u0026#34; { default = \u0026#34;az-ter\u0026#34; } variable \u0026#34;ssh_public_key\u0026#34;{ default = \u0026#34;./id_rsa.pub\u0026#34; } Creating outputs to be sent back after Terraform finishes running Create a file named output.tf. Certain parts of the output file are critical to get access to the cluster once it is spun up. Specifically the cluster_name, and resource_group_name outputs are essential.These will be used in order to obtain kubectl access for managing the cluster. I included a few extra output resources for future usage.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 output \u0026#34;client_key\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.client_key sensitive = true } output \u0026#34;client_certificate\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.client_certificate sensitive = true } output \u0026#34;cluster_ca_certificate\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.cluster_ca_certificate sensitive = true } output \u0026#34;cluster_username\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.username sensitive = true } output \u0026#34;cluster_password\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.password sensitive = true } output \u0026#34;kube_config\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config_raw sensitive = true } output \u0026#34;host\u0026#34; { value = azurerm_kubernetes_cluster.cluster.kube_config.0.host sensitive = true } # Critical to get kubectl file connected out to Azure for local environment output \u0026#34;cluster_name\u0026#34; { value = azurerm_kubernetes_cluster.cluster.name } output \u0026#34;resource_group_name\u0026#34; { value = azurerm_resource_group.cluster.name } Gain access to Kubectl In order to gain access from your local machine, we will use the azure CLI. If you followed the tutorial, you\u0026rsquo;ll already be logged in, az login. Use the following command to set your environment varialbe for kubectl to control kubernetes cluster in Azure. az aks get-credentials --resource-group $(terraform output -raw resource_group_name) --name $(terraform output -raw cluster_name)\nOnce ran, you can verify it is connected and working with kubectl get nodes , kubectl get namespace\nUseful Resources Kubernetes Overview Terraform Kubernetes Terraform Azurerm ","date":"2022-04-04T00:00:00Z","image":"https://cinderblock.github.io/p/terraform-azure-kubernetes-deployment/Terraform-DeployingKubernetesAKS_hua55605408eff8611cfa14e7c2dd7cd7c_981428_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/terraform-azure-kubernetes-deployment/","title":"Terraform - Azure Kubernetes Deployment"},{"content":"Overview Getting started with K3S: The primary goal here is to setup a functional highly available K3S cluster. This will include 4 necessary steps:\nSetup NGINX Loadbalancer Docker Setup MySQL Docker Setup Highly Available K3s Cluster (Optional) Setup management from dev machine (Controller) Setup Rancher as a container within the cluster Check out all of the configuration files on GitHub (k3s-HACluster-Rancher) at the repository!\nPrerequisites Have a dedicated Docker host virtual machine, preferrably linux Have 5 Linux virtual machines ready Two will be Master Nodes, and Three will be worker nodes. Each will have a dedicated IP address. I personally ran all of my linux Virtual Machines as Ubuntu Server 20.04\n1. Setup NGINX Loadbalancer Docker Log into your dedicated docker linux host and create a NGINX Loadbalancer using docker Ensure docker-compose is installed\nsudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose After setup, create a directory for nginx, and create a file called docker-compose.yml with the following contents\n1 2 3 4 5 6 7 8 9 10 11 12 13 version: \u0026#39;3\u0026#39; services: nginx: image: nginx:latest container_name: nginx volumes: - ./nginx.conf:/etc/nginx/nginx.conf ports: - 80:80 - 443:443 - 6443:6443 restart: on-failure In the same directorty, create a nginx.conf file with the following contet: change \u0026lt;IP_MASTER_NODE1 \u0026amp; 2\u0026gt; to your two node IP addresses. Change \u0026lt;IP_NODE_1,2,3\u0026gt; to Worker Node IPs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 events{} stream { upstream k3s_servers { server \u0026lt;IP_MASTER_NODE_1\u0026gt;:6443; server \u0026lt;IP_MASTER_NODE_2\u0026gt;:6443; } server { listen 6443; proxy_pass k3s_servers; } upstream rancher_servers_http { least_conn; server \u0026lt;IP_NODE_1\u0026gt;:80 max_fails=3 fail_timeout=5s; server \u0026lt;IP_NODE_2\u0026gt;:80 max_fails=3 fail_timeout=5s; server \u0026lt;IP_NODE_3\u0026gt;:80 max_fails=3 fail_timeout=5s; } server { listen 80; proxy_pass rancher_servers_http; } upstream rancher_servers_https { least_conn; server \u0026lt;IP_NODE_1\u0026gt;:443 max_fails=3 fail_timeout=5s; server \u0026lt;IP_NODE_2\u0026gt;:443 max_fails=3 fail_timeout=5s; server \u0026lt;IP_NODE_3\u0026gt;:443 max_fails=3 fail_timeout=5s; } server { listen 443; proxy_pass rancher_servers_https; } } Commands to setup: Change nginx.conf file to match your configuration Enter file directory of nginx and apply sudo docker-compose up -d 2. Setup MySQL Docker On the same dedicated docker host that the nginx loadbalancer is running on: Create new directory for mysql and put a docker-compose.yml file in it with the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- version: \u0026#39;3.1\u0026#39; services: mysql: image: mysql:latest restart: always container_name: mysql ports: - 3306:3306 expose: - \u0026#34;3306\u0026#34; volumes: - ./mysql-data:/var/lib/mysql/ environment: - MYSQL_ROOT_PASSWORD=enter-your-password Commands to setup:\nEnter file directory of mysql and apply sudo docker-compose up -d Enter the docker to execute commands sudo docker exec -it mysql bash mysql -p (Enter password) Next, create the database and assign a user to it to be used with the K3s cluster within the mysql bash shell using the following SQL commands. Change \u0026lsquo;password\u0026rsquo; and \u0026lsquo;user\u0026rsquo; to your desired variables 1 2 3 4 CREATE DATABASE k3s COLLATE latin1_swedish_ci; CREATE USER user@% IDENTIFIED BY password; GRANT ALL ON k3s.* TO \u0026#39;user\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; To Note: the Database MUST be latin1_swedish_ci\n3. Setup Highly Available K3s Cluster These will be setup in three steps. The first, setting up the first master node in the K3s cluster. Then, joining an additional master node. Finally, adding the worker nodes.\n1. Primary Master Node setup On the first Server node run these commands:\nexport K3S_DATASTORE_ENDPOINT='mysql://user:password@tcp(sqlhost:3306)/database-name Change values to your database values. \u0026lsquo;user\u0026rsquo;, \u0026lsquo;password\u0026rsquo;, \u0026lsquo;sqlhost\u0026rsquo;, \u0026lsquo;database-name\u0026rsquo; curl -sfL https://get.k3s.io | sh -s - server --node-taint CriticalAddonsOnly=true:NoExecute --tls-san 'Load-Balancer-Address' After it has connected and you can successfully, check and ensure you can see the node with sudo kubectl get nodes\nObtain node-token from sudo cat /var/lib/rancher/k3s/server/node-token This will be used in the next steps to join the second master node and the workers 2. Secondary Master Node setup On additional server nodes:\nexport K3S_DATASTORE_ENDPOINT='mysql://user:password@tcp(sqlhost:3306)/database-name Change values to your database values. \u0026lsquo;user\u0026rsquo;, \u0026lsquo;password\u0026rsquo;, \u0026lsquo;sqlhost\u0026rsquo;, \u0026lsquo;database-name\u0026rsquo; Curl -sfL https://get.k3s.io | sh -s - server --node-taint CriticalAddonsOnly=true:NoExecute --tls-san 'Load-Balancer-Address' --token server-token-here sh - Change values to your database values. \u0026lsquo;Load-Balancer-Address\u0026rsquo;, \u0026lsquo;server-token-here\u0026rsquo; 3. Worker Node Setup On all client agents to be added to cluster\nexport K3S_DATASTORE_ENDPOINT='mysql://user:password@tcp(sqlhost:3306)/database-name' Change values to your database values. \u0026lsquo;user\u0026rsquo;, \u0026lsquo;password\u0026rsquo;, \u0026lsquo;sqlhost\u0026rsquo;, \u0026lsquo;database-name\u0026rsquo; curl -sfL https://get.k3s.io | K3S_URL=https://'Load-Balance-Address':6443 K3S_TOKEN=server-token-here sh - Change values to your database values. \u0026lsquo;Load-Balancer-Address\u0026rsquo;, \u0026lsquo;K3S_TOKEN=server-token-here\u0026rsquo; 4. (Optional) Setup management from dev machine (Controller) Once Cluster has been setup\ninstall k3s on controller machine w/ curl -LO \u0026quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026quot; On a server node run: sudo nano /etc/rancher/k3s/k3s.yaml Copy contents of that file to a file on your dev machine at location /home/user/.kube/config Change IP address in config to match your Load Balancer verify it is working w/ kubectl cluster-info You can now control your K3s cluster from another machine, outside of the cluster!\n5. Setup Rancher as a container within the cluster Deploying Rancher on Workers using Helm This deployment will be using the self-generated Rancher Certificate. Either from a Master Node, or the Controller machine (If you followed step 4):\nAdd the Helm Chart Repository\nInstall helm curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Use Stable release: helm repo add rancher-stable https://releases.rancher.com/server-charts/stable Create Namespace within K3s for Rancher\nkubectl create namespace cattle-system Install Cert Manager (Required for self-hosted cert)\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml helm repo add jetstack https://charts.jetstack.io helm repo update 1 2 3 4 helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.1 If you experience an issue running this, such as localhost:8080 error, add KUBECONFIG as an environment variable to fix it: export KUBECONFIG=/etc/rancher/k3s/k3s.yaml If that does not fix it, ensure the .kube config file exists in the proper locaiton kubectl config view --raw \u0026gt; ~/.kube/config Verify it is working w/ kubectl get pods --namespace cert-manager Install Rancher using Helm\n1 2 3 4 5 helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=port.lan \\ --set replicas=3 \\ --set bootstrapPassword=password Check on deployment w/ kubectl -n cattle-system rollout status deploy/rancher Once finished, obtain info on deployment w/ kubectl -n cattle-system get deploy rancher Once Rancher is deployed\nNavigate to {LoadBalancer-DNS} site It must be the DNS entry of the Load Balancer due to the certification. An IP adddress will NOT work Find your secret using command given at site login, and log into the site Bam, Rancher installed, and it is now highly available.\nTroubeshooting Rancher If you must uninstall and reinstall Rancher for any reason, I recommend these steps (They are painful) For the name spaces affecting Rancher Directly, sudo kubectl delete namespace namespace-name Check it w/ kubectl get ns If stuck in terminating, kubectl edit ns namespace-name Delete everything under the finalizer: fields (Sometimes there are two.) Useful Resources \u0026amp; Commands Useful Docker Commands The load balancer and database are setup using Docker-compose Useful Docker compose commands: 1 2 3 4 5 6 7 8 9 10 # Sping up a docker based on docker-compose.yml file docker-compose up -f \u0026#39;filenamehere\u0026#39; # Sping up a docker and keep it running docker-compose up -d # Find list of running docker processes docker ps # Get into shell of a running docker docker exec -it [option] bash # Update running docker with new configuration changes docker-compose up -d --force-recreate Useful Kubectl Commands Kubernetes commands (Mostly pulled from kubectl cheat sheet) Finding ressources 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Get commands with basic output bectl config view # Show Merged kubeconfig settings. beclt get nodes\t# Show all nodes in the cluster bectl get services # List all services in the namespace bectl get pods --all-namespaces # List all pods in all namespaces bectl get pods -o wide # List all pods in the current namespace, with more details bectl get deployment my-dep # List a particular deployment bectl get pods # List all pods in the namespace bectl get pod my-pod -o yaml # Get a pod\u0026#39;s YAML``` Describe commands with verbose output bectl describe nodes my-node bectl describe pods my-pod List Services Sorted by Name bectl get services --sort-by=.metadata.name Updating resources 1 2 3 4 5 6 kubectl set image deployment/frontend www=image:v2 # Rolling update \u0026#34;www\u0026#34; containers of \u0026#34;frontend\u0026#34; deployment, updating the image kubectl rollout history deployment/frontend # Check the history of deployments including the revision kubectl rollout undo deployment/frontend # Rollback to the previous deployment kubectl rollout undo deployment/frontend --to-revision=2 # Rollback to a specific revision kubectl rollout status -w deployment/frontend # Watch rolling update status of \u0026#34;frontend\u0026#34; deployment until completion kubectl rollout restart deployment/frontend # Rolling restart of the \u0026#34;frontend\u0026#34; deployment Allow kubectl without sudo priviledge sudo chmod 644 /etc/rancher/k3s/k3s.yaml Resources Shout out to TheQuib I thank him for his collaboration on this Rancher Documentation Docker Compose Documentation ","date":"2022-03-06T00:00:00Z","image":"https://cinderblock.github.io/p/k3s-highly-available-rancher/k3sRancher_huef3ebd6935b6086b2675dbc1fcecea14_671407_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/k3s-highly-available-rancher/","title":"K3S - Highly-Available-Rancher"},{"content":"Overview Deploy and Configure 4 Windows 2022 Datacenter Servers in Azure.\nUsing Terraform in conjunction with Ansible and cloudinit: Create 4 Windows Servers Configure them to be a Primary Domain Controller, Replica Domain Controller, DHCP server, and Fileshare server Automate intial setup of the 4 servers to accept Ansible configuration from a Linux VM in Azure created VIA the Terraform deployment Check out all of the configuration files on GitHub (Azure-Serv-Deploy) at the repository!\nTerraform Main role: Deploy the Virtual Machines, setup Network environment, and provide intial parameters for both Windows and Linux environments running in the cloud\nSetup the four Windows Servers (Primary Domain Controller, Replica Domain Controller, DHCP, Fileshare) in Azure These will all be Windows 2022 Datacenter Servers running on Standard_DS1_V2 by default Setup the one Linux server to deploy a pre-defined Ansible configuration across the Windows Environment for setting up Active Directory, DHCP, File shares, users, and groups. This will all be an Ubuntu 18.04-LTS server running on Standard_B1s by default It will use cloud-init to supply it the necessary setup at creation for Ansible and SSH connection VIA its public IP address. Supply necessary networking variables (Network interfaces, Security Groups, IP Addressing) Supply necessary files for automation of Windows \u0026amp; Linux environments (Cloud-Init \u0026amp; Windows Unattend files) Prerequisites Setup necessary Terraform environment Install and setup Azure CLI or preferred method of authentication to Azure Configure variables for desired outcomes (Outlined further down) Terraform process Using the Azure provider: Login to Azure with az connect Once prepared with appropriate values and the networking is in place: Navigate to the Terraform directory and run these commands terraform init Pull proper Terraform providers and modules used terraform validate This will return whether the configuration is valid or not terraform apply \u0026hellip; yes Actually apply the configuration Terraform File Structure Create a new directory, and place the following files in it, with your own variables\nprovider.tf File Calls necessary providers and sets their versions to be used in the Terraform configuration/deployment. Informs Terraform which modules and providers to use. provider.tf\n1 2 3 4 5 6 7 8 9 10 11 terraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;\u0026gt;=2.91.0\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} } networking.tf File Defines resources, security groups, security rules, network interfaces, subnets, and public IPs to be created in Azure. These variables are pulled from the VM creation resources Managed with variables contained in terraform.tfvars file networking.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 # Create a resource group to maintain security settings along with network interfaces for VMs resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;east\u0026#34; { name = \u0026#34;terra-resources\u0026#34; location = \u0026#34;East US\u0026#34; } # ASSIGN ADDRESS SPACE TO RESOURCE GROUP resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;east\u0026#34; { name = \u0026#34;east-network\u0026#34; address_space = [\u0026#34;${var.east_address_spaces}\u0026#34;] location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name } # ASSIGN SUBNET TO NETWORK ADDRESS SPACE resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;subnet1\u0026#34; { name = \u0026#34;internal\u0026#34; resource_group_name = azurerm_resource_group.east.name virtual_network_name = azurerm_virtual_network.east.name address_prefixes = [var.east_subnets] } # Create public IP variable for Linux machine resource \u0026#34;azurerm_public_ip\u0026#34; \u0026#34;linux_public\u0026#34; { name = \u0026#34;PublicIp1\u0026#34; resource_group_name = azurerm_resource_group.east.name location = azurerm_resource_group.east.location allocation_method = \u0026#34;Static\u0026#34; } # Create public IP variable for Windows machine resource \u0026#34;azurerm_public_ip\u0026#34; \u0026#34;win_public\u0026#34; { name = \u0026#34;PublicIp2\u0026#34; resource_group_name = azurerm_resource_group.east.name location = azurerm_resource_group.east.location allocation_method = \u0026#34;Static\u0026#34; } # ASSIGN NETWORK INTERFACE PER VM WE WILL BE USING resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;linux1\u0026#34; { name = \u0026#34;linux1-nic\u0026#34; location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name ip_configuration { name = \u0026#34;internal\u0026#34; subnet_id = azurerm_subnet.subnet1.id private_ip_address_allocation = \u0026#34;Static\u0026#34; private_ip_address = var.linux1_priavte_ip public_ip_address_id = azurerm_public_ip.linux_public.id } } resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;winserv1\u0026#34; { name = \u0026#34;winserv1-nic\u0026#34; location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name ip_configuration { name = \u0026#34;internal\u0026#34; subnet_id = azurerm_subnet.subnet1.id private_ip_address_allocation = \u0026#34;Static\u0026#34; private_ip_address = var.winserv1_private_ip public_ip_address_id = azurerm_public_ip.win_public.id } } resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;winserv2\u0026#34; { name = \u0026#34;winserv2-nic\u0026#34; location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name ip_configuration { name = \u0026#34;internal\u0026#34; subnet_id = azurerm_subnet.subnet1.id private_ip_address_allocation = \u0026#34;Static\u0026#34; private_ip_address = var.winserv2_private_ip } } resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;winserv3\u0026#34; { name = \u0026#34;winserv3-nic\u0026#34; location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name ip_configuration { name = \u0026#34;internal\u0026#34; subnet_id = azurerm_subnet.subnet1.id private_ip_address_allocation = \u0026#34;Static\u0026#34; private_ip_address = var.winserv3_private_ip } } resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;winserv4\u0026#34; { name = \u0026#34;winserv4-nic\u0026#34; location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name ip_configuration { name = \u0026#34;internal\u0026#34; subnet_id = azurerm_subnet.subnet1.id private_ip_address_allocation = \u0026#34;Static\u0026#34; private_ip_address = var.winserv4_private_ip } } # CREATE SECURITY GROUPs TO ALLOW SSH/RDP/ANSIBLE FOR VMs resource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;linux1\u0026#34; { name = \u0026#34;Allow-SSH\u0026#34; location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name security_rule { name = \u0026#34;SSH\u0026#34; priority = 100 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;22\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } } resource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;winserv\u0026#34; { name = \u0026#34;Allow-RDP-SSH-ANS\u0026#34; location = azurerm_resource_group.east.location resource_group_name = azurerm_resource_group.east.name security_rule { name = \u0026#34;RDP\u0026#34; priority = 101 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;3389\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { name = \u0026#34;SSH\u0026#34; priority = 102 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;22\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { name = \u0026#34;ANSIBLE\u0026#34; priority = 103 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;5985\u0026#34; source_address_prefix = \u0026#34;${var.east_subnets}\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } } # ASSIGN SECURITY GROUPS TO INTERFACES # LINUX SSH resource \u0026#34;azurerm_network_interface_security_group_association\u0026#34; \u0026#34;linux1\u0026#34; { network_interface_id = azurerm_network_interface.linux1.id network_security_group_id = azurerm_network_security_group.linux1.id } # WINSERV RDP resource \u0026#34;azurerm_network_interface_security_group_association\u0026#34; \u0026#34;winserv1\u0026#34; { network_interface_id = azurerm_network_interface.winserv1.id network_security_group_id = azurerm_network_security_group.winserv.id } resource \u0026#34;azurerm_network_interface_security_group_association\u0026#34; \u0026#34;winserv2\u0026#34; { network_interface_id = azurerm_network_interface.winserv2.id network_security_group_id = azurerm_network_security_group.winserv.id } resource \u0026#34;azurerm_network_interface_security_group_association\u0026#34; \u0026#34;winserv3\u0026#34; { network_interface_id = azurerm_network_interface.winserv3.id network_security_group_id = azurerm_network_security_group.winserv.id } resource \u0026#34;azurerm_network_interface_security_group_association\u0026#34; \u0026#34;winserv4\u0026#34; { network_interface_id = azurerm_network_interface.winserv4.id network_security_group_id = azurerm_network_security_group.winserv.id } variables.tf, terraform.tfvars Files Alter variables within these files to ensure they meet your environment needs variables.tf Declare variables that will be used with the Terraform configuration (Delcared intially or explicitely here as locals variables) Specific local variables used for Windows .xml files firsT_logon_commands local variable points to a .xml file to configure first time logon in Windows. This enables each server to recieve Winrm data on port 5985 for Ansible configuration *auto_logon_ runs a .xml configuration to log in once right after intial creation of VM. This allows first_logon_commands to execute automatically variables.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 variable \u0026#34;winserv_vm_os_publisher\u0026#34; {} variable \u0026#34;winserv_vm_os_offer\u0026#34; {} variable \u0026#34;winserv_vm_os_sku\u0026#34; {} variable \u0026#34;winserv_vm_size\u0026#34; {} variable \u0026#34;winadmin_username\u0026#34; {} variable \u0026#34;winadmin_password\u0026#34; {} variable \u0026#34;winserv_license\u0026#34; {} variable \u0026#34;winserv_sa_type\u0026#34; {} variable \u0026#34;winserv_pdc\u0026#34; {} variable \u0026#34;winserv_rdc\u0026#34; {} variable \u0026#34;winserv_dhcp\u0026#34; {} variable \u0026#34;winserv_file\u0026#34; {} variable \u0026#34;linux_server\u0026#34; {} variable \u0026#34;linux_vm_os_publisher\u0026#34; {} variable \u0026#34;linux_vm_os_offer\u0026#34; {} variable \u0026#34;linux_vm_os_sku\u0026#34; {} variable \u0026#34;linux_vm_size\u0026#34; {} variable \u0026#34;linux_ssh_key\u0026#34; {} variable \u0026#34;linux_sa_type\u0026#34; {} variable \u0026#34;linux_ssh_key_pv\u0026#34; {} variable \u0026#34;winserv_allocation_method\u0026#34; {} variable \u0026#34;east_address_spaces\u0026#34; {} variable \u0026#34;east_subnets\u0026#34; {} variable \u0026#34;winserv_public_ip_sku\u0026#34; {} variable \u0026#34;winserv1_private_ip\u0026#34; {} variable \u0026#34;winserv2_private_ip\u0026#34; {} variable \u0026#34;winserv3_private_ip\u0026#34; {} variable \u0026#34;winserv4_private_ip\u0026#34; {} variable \u0026#34;linux1_priavte_ip\u0026#34; {} locals{ first_logon_commands = file(\u0026#34;${path.module}/ winfiles/FirstLogonCommands.xml\u0026#34;) auto_logon = \u0026#34;\u0026lt;AutoLogon\u0026gt;\u0026lt;Password\u0026gt;\u0026lt;Value\u0026gt;${var.winadmin_password}\u0026lt;/ Value\u0026gt;\u0026lt;/Password\u0026gt;\u0026lt;Enabled\u0026gt;true\u0026lt;/Enabled\u0026gt;\u0026lt;LogonCount\u0026gt;1\u0026lt;/ LogonCount\u0026gt;\u0026lt;Username\u0026gt;${var.winadmin_username}\u0026lt;/ Username\u0026gt;\u0026lt;/AutoLogon\u0026gt;\u0026#34; } terraform.tfvars Assign variables values here. These will be used with the Terraform configuration. If left blank, you can assign the variable at the terminal level when running the terraform apply Alter Network values to desired IP addressing scheme Ensure IP addressing matches that in the Ansible configuration inventory.yml Here you can alter azure values for publisher, offer, sku, size, sa, and license information for the Windows/Linux VMs Additionally, ensure linux_ssh_key point to your public Key id_rsa.pubc file I recommend to change winadmin_username \u0026amp; winadmin_password variables to sensetive and blank so you can delcare them in preferrably Vaulty or via the CLI winadmin_username \u0026amp; winadmin_password MUST MATCH WHAT IS IN ANSIBLE /group_vars/all.yml terraform.tfvars\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Azure Windows Server related params winserv_vm_os_publisher =\u0026#34;MicrosoftWindowsServer\u0026#34; winserv_vm_os_offer = \u0026#34;WindowsServer\u0026#34; winserv_vm_os_sku = \u0026#34;2022-Datacenter\u0026#34; winserv_vm_size = \u0026#34;Standard_DS1_V2\u0026#34; winserv_license = \u0026#34;Windows_Server\u0026#34; winserv_allocation_method = \u0026#34;Static\u0026#34; winserv_public_ip_sku = \u0026#34;Standard\u0026#34; winserv_sa_type = \u0026#34;Standard_LRS\u0026#34; # Azure Linux Server related params linux_vm_os_publisher = \u0026#34;Canonical\u0026#34; linux_vm_os_offer = \u0026#34;UbuntuServer\u0026#34; linux_vm_os_sku = \u0026#34;18.04-LTS\u0026#34; linux_vm_size = \u0026#34;Standard_B1s\u0026#34; linux_ssh_key =\u0026#34;Local-PUBLIC-SSH-KEY-Here\u0026#34; linux_sa_type = \u0026#34;Premium_LRS\u0026#34; linux_ssh_key_pv = \u0026#34;Local-PRIV-SSH-KEY-Here\u0026#34; # Which Windows administrator password to setduring vm customization winadmin_username = \u0026#34;SuperAdministrator\u0026#34; winadmin_password = \u0026#34;Password1234\u0026#34; # Naming Schemes winserv_pdc = \u0026#34;ajb-pdc\u0026#34; winserv_rdc = \u0026#34;ajb-rdc\u0026#34; winserv_dhcp = \u0026#34;ajb-dhcp\u0026#34; winserv_file = \u0026#34;ajb-file\u0026#34; linux_server = \u0026#34;ajb-operator\u0026#34; # Networking Variables winserv1_private_ip = \u0026#34;10.0.1.10\u0026#34; winserv2_private_ip = \u0026#34;10.0.1.11\u0026#34; winserv3_private_ip = \u0026#34;10.0.1.12\u0026#34; winserv4_private_ip = \u0026#34;10.0.1.13\u0026#34; linux1_priavte_ip = \u0026#34;10.0.1.9\u0026#34; east_address_spaces = \u0026#34;10.0.0.0/16\u0026#34; east_subnets = \u0026#34;10.0.1.0/24\u0026#34; 01-LinuxClient.tf \u0026amp; 02-WinServers.tf Files Here the creation of the VMs occur. Resources pull data from networking.tf, variables.tf, terraform.tfvars files. Windows VMs are assigned unattend configurations for first time setup (/winfiles/FirstLogonCommands.xml \u0026amp;\u0026amp; auto_logon variable data) Linux Machine is assigned a cloud-init file configuraiton for first time setup (/cloudinit/custom.yml) We will create this in the next step. 01-LinuxClient.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 # This pulls a Ubuntu Datacenter from Microsoft\u0026#39;s VM platform directly resource \u0026#34;azurerm_linux_virtual_machine\u0026#34; \u0026#34;operator\u0026#34; { name = var.linux_server resource_group_name = azurerm_resource_group.east.name location = azurerm_resource_group.east.location size = var.linux_vm_size admin_username = var.winadmin_username network_interface_ids = [ azurerm_network_interface.linux1.id ] admin_ssh_key { username = var.winadmin_username public_key = file(\u0026#34;${var.linux_ssh_key}\u0026#34;) } # Cloud-Init passed here custom_data = data.template_cloudinit_config.config.rendered os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = var.linux_sa_type } source_image_reference { publisher = var.linux_vm_os_publisher offer = var.linux_vm_os_offer sku = var.linux_vm_os_sku version = \u0026#34;latest\u0026#34; } depends_on = [azurerm_resource_group.east, azurerm_network_interface.linux1] } # Create cloud-init file to be passed into linux vm data \u0026#34;template_file\u0026#34; \u0026#34;user_data\u0026#34; { template = file(\u0026#34;./cloudinit/custom.yml\u0026#34;) } # Render a multi-part cloud-init config making use of the part # above, and other source files data \u0026#34;template_cloudinit_config\u0026#34; \u0026#34;config\u0026#34; { gzip = true base64_encode = true # Main cloud-config configuration file. part { filename = \u0026#34;init.cfg\u0026#34; content_type = \u0026#34;text/cloud-config\u0026#34; content = \u0026#34;${data.template_file.user_data.rendered}\u0026#34; } } # Pass Ansible File into created Linux VM using SCP (SSH Port 22) resource \u0026#34;null_resource\u0026#34; \u0026#34;copyansible\u0026#34;{ connection { type = \u0026#34;ssh\u0026#34; host = azurerm_public_ip.linux_public.ip_address user = var.winadmin_username private_key = file(\u0026#34;${var.linux_ssh_key_pv}\u0026#34;) } provisioner \u0026#34;file\u0026#34; { source = \u0026#34;${path.module}/Ansible\u0026#34; destination = \u0026#34;/tmp/\u0026#34; } depends_on = [azurerm_linux_virtual_machine.operator] } 02-WinServers.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 # This pulls the latest Windows Server Datacenter from Microsoft\u0026#39;s VM platform directly resource \u0026#34;azurerm_windows_virtual_machine\u0026#34; \u0026#34;pdc\u0026#34; { name = var.winserv_pdc resource_group_name = azurerm_resource_group.east.name location = azurerm_resource_group.east.location size = var.winserv_vm_size admin_username = var.winadmin_username admin_password = var.winadmin_password network_interface_ids = [ azurerm_network_interface.winserv1.id ] os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = var.winserv_sa_type } source_image_reference { publisher = var.winserv_vm_os_publisher offer = var.winserv_vm_os_offer sku = var.winserv_vm_os_sku version = \u0026#34;latest\u0026#34; } additional_unattend_content { content = local.auto_logon setting = \u0026#34;AutoLogon\u0026#34; } additional_unattend_content { content = local.first_logon_commands setting = \u0026#34;FirstLogonCommands\u0026#34; } depends_on = [azurerm_resource_group.east, azurerm_network_interface.winserv1] } resource \u0026#34;azurerm_windows_virtual_machine\u0026#34; \u0026#34;rdc\u0026#34; { name = var.winserv_rdc resource_group_name = azurerm_resource_group.east.name location = azurerm_resource_group.east.location size = var.winserv_vm_size admin_username = var.winadmin_username admin_password = var.winadmin_password network_interface_ids = [ azurerm_network_interface.winserv2.id ] os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = var.winserv_sa_type } source_image_reference { publisher = var.winserv_vm_os_publisher offer = var.winserv_vm_os_offer sku = var.winserv_vm_os_sku version = \u0026#34;latest\u0026#34; } additional_unattend_content { content = local.auto_logon setting = \u0026#34;AutoLogon\u0026#34; } additional_unattend_content { content = local.first_logon_commands setting = \u0026#34;FirstLogonCommands\u0026#34; } depends_on = [azurerm_resource_group.east, azurerm_network_interface.winserv2] } resource \u0026#34;azurerm_windows_virtual_machine\u0026#34; \u0026#34;dhcp\u0026#34; { name = var.winserv_dhcp resource_group_name = azurerm_resource_group.east.name location = azurerm_resource_group.east.location size = var.winserv_vm_size admin_username = var.winadmin_username admin_password = var.winadmin_password network_interface_ids = [ azurerm_network_interface.winserv3.id ] os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = var.winserv_sa_type } source_image_reference { publisher = var.winserv_vm_os_publisher offer = var.winserv_vm_os_offer sku = var.winserv_vm_os_sku version = \u0026#34;latest\u0026#34; } additional_unattend_content { content = local.auto_logon setting = \u0026#34;AutoLogon\u0026#34; } additional_unattend_content { content = local.first_logon_commands setting = \u0026#34;FirstLogonCommands\u0026#34; } depends_on = [azurerm_resource_group.east, azurerm_network_interface.winserv3] } resource \u0026#34;azurerm_windows_virtual_machine\u0026#34; \u0026#34;file\u0026#34; { name = var.winserv_file resource_group_name = azurerm_resource_group.east.name location = azurerm_resource_group.east.location size = var.winserv_vm_size admin_username = var.winadmin_username admin_password = var.winadmin_password network_interface_ids = [ azurerm_network_interface.winserv4.id ] os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = var.winserv_sa_type } source_image_reference { publisher = var.winserv_vm_os_publisher offer = var.winserv_vm_os_offer sku = var.winserv_vm_os_sku version = \u0026#34;latest\u0026#34; } additional_unattend_content { content = local.auto_logon setting = \u0026#34;AutoLogon\u0026#34; } additional_unattend_content { content = local.first_logon_commands setting = \u0026#34;FirstLogonCommands\u0026#34; } depends_on = [azurerm_resource_group.east, azurerm_network_interface.winserv4] } outputs.tf File Provides necessary ip information that is allocated to the VMs created. This information by default includes: Private IPs for all 5 deployed VMs (Which we know will by based on variables.tf file data) Public IP for Linux machine (Not known by default, will be used for SSH connection if needed). outputs.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 output \u0026#34;Public_IP_Linux\u0026#34; { value = azurerm_public_ip.linux_public.ip_address } output \u0026#34;Public_IP_Windows\u0026#34; { value = azurerm_public_ip.win_public.ip_address } output \u0026#34;Private_IP_Linux\u0026#34; { value = azurerm_network_interface.linux1.private_ip_address } output \u0026#34;Private_IP_WinServ\u0026#34; { value = [ \u0026#34;PDC: ${azurerm_windows_virtual_machine.pdc.private_ip_address}\u0026#34;, \u0026#34;RDC: ${azurerm_windows_virtual_machine.rdc.private_ip_address}\u0026#34;, \u0026#34;DHCP: ${azurerm_windows_virtual_machine.dhcp.private_ip_address}\u0026#34;, \u0026#34;FILE: ${azurerm_windows_virtual_machine.file.private_ip_address}\u0026#34; ] } Cloud-init Notice that in the 01-linux file, there is a cloud-init section, where the spun up machine will pull a cloud-init file. We must create that. Make a a folder inside your root terraform directory. Name it cloudinit. Inside that folder, create a file called custom.yml containing the following.\n1 2 3 4 5 6 7 8 9 10 11 #cloud-config apt_update: true packages: - python-pip runcmd: - sudo pip install ansible - sudo ansible-galaxy install azure.azure_preview_modules - sudo pip install -r ~/.ansible/roles/azure.azure_preview_modules/files/requirements-azure.txt - pip install \u0026#34;pywinrm\u0026gt;=0.2.2\u0026#34; - cd /tmp/Ansible - sudo ansible-playbook winlab.yml Useful Azure related functions Finding variable information for VM Images variables:\nYou can use this command in Azure CLI to find UbuntuServer data. Change the values in offer, publisher, location, and sku for various other images. 1 2 3 4 5 6 az vm image list \\ --location westus \\ --publisher Canonical \\ --offer UbuntuServer \\ --sku 18.04-LTS \\ --all --output table \u0026ldquo;Check out Microsoft\u0026rsquo;s\u0026rdquo; documentation on finding VM information Ansible Main role: Configure the deployed Virtual Machines.\nCheck out the repository on GitHub for the configuration files!\nSetup Windows Server Feature: Domain Primary Domain Controller Replica Domain Controller Auto-Join the Virutal Machines to the respective Domain created Create a few users and groups within Active Directory Setup Windows Ssrver Feature: DHCP Setup DHCP Scope Authorize it to the Domain. Setup Windows Server Feature: File Sharing Create two shares An employee share and administrator share. These shares are assigned group permissions. Common Configurations Enable RDP and allow it through the firewall on all windows servers created at server level Ansible Variable files inventory.yml Modify hosts associated with the playbook. Assign the IP addressing MUST MATCH terraform.tfvars VARIABLE IP ADDRESSING winlab.yml Associate \u0026lsquo;roles\u0026rsquo; to the hosts identified in the inventory file. These \u0026lsquo;roles\u0026rsquo; are folders within the directory containing a set of code to configure per host ansible.cfg Tells ansible variable information. In this scenario, identifies to use inventory.yml file. ./group_vars/all.yml Contains specific variable information used within the ./roles/* Ansible code. Alter user, password, port, connection, and cert variable information Alter domain variables as well Running Ansible This is taken care of with terraform cloud-init file along with the file provisioner. The alternative would be below.\nOn Linux Machine, Requires: Python-pip, ansible-galaxzy-azure.azure_preview_modules To Run: Navigate to Ansible directory and type ansible-playbook winlab.yml Useful Resources Terraform Resources Terraform Documentation Azure Provider \u0026amp; Modules Cloud-init Documentation terraform-provider-azurerm examples and documentation on GitHub Ansible Resources Ansible Documentation Windows-Modules ","date":"2022-02-28T00:00:00Z","image":"https://cinderblock.github.io/p/terraform-azure-server-deploy/Azure\u0026Terraform_huba542f586351a1d31609965a810d8cb5_154659_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/terraform-azure-server-deploy/","title":"Terraform - Azure Server Deploy"},{"content":"Overview The goal of this project is to deploy a ready-to-go windows server environment. This includes a domain controller, a replica domain controller, a DHCP server, and a fileserver. Additionally setting up users, groups, and OUs for the respective users within the domain. To complete this project, 3 steps are taken.\nUse Packer to spin up a sys prepped and fully updated windows server 2022 iso for the environemnt Use Terraform to deploy 4 virtual machines into a vSphere environment Use Ansible to configure these 4 virtual machines as desired Check out all of the configuration files on GitHub (vSphere-WinServ-Deployment) at the repository!\n1. Packer\u0026rsquo;s Role: Create a Windows Server 2022 .iso that is updated and has VMTools installed by default using Packer. In this solution, it will be geared to usage with vSphere, a VMware product.\nFirst: Packer uses autounattend.xml and sysprep-autounattend.xml to automate Windows Settings\nIt pulls Windows Server 2022 Datacenter Eval Edition (Desktop Experience) from Microsoft\u0026rsquo;s site Installs \u0026amp; configure OpenSSH Client \u0026amp; Server for remote connection Installs VMware tools from ISO provided from the build ESX server Packer Provisioner Steps\nUpdating OS via Windows Update Doing some OS adjustments Set Windows telemetry settings to minimum Show file extentions by default Install Chocolatey - a Windows package manager Install Microsoft Edge (Chromium) Install Win32-OpenSSH-Server Install PowerShell Core Install 7-Zip Install Notepad++ Enable Powershell-Core (pwsh) to be the default SSHD shell Cleanup tasks Remove CDROM drives from VM template (otherwise there would be 2) 2. Terraform\u0026rsquo;s Role: Main role: Deploy the Virtual Machines\nSetup the four Windows Servers (Primary Domain Controller, Replica Domain Controller, DHCP, Fileshare) Using the vSphere provider: Assign appropriate resources to each machine Once prepared with appropriate values and the networking is in place: Navigate to the Terraform directory and run these commands terraform init Pull proper Terraform providers and modules used terraform validate This will return whether the configuration is valid or not terraform apply \u0026hellip; yes Actually apply the configuration Terraform Variable files variables.tf Declare variables that will be used with the Terraform configuration terraform.tfvars Assign variables that will be used with the Terraform configuration 3. Ansible\u0026rsquo;s Role: Main role: Configure the deployed Virtual Machines.\nSetup Windows Server Feature: Domain Primary Domain Controller Replica Domain Controller Auto-Join the Virutal Machines to the respective Domain created Create a few users and groups within Active Directory Setup Windows Ssrver Feature: DHCP Setup DHCP Scope Authorize it to the Domain. Setup Windows Server Feature: File Sharing Create two shares An employee share and administrator share. These shares are assigned group permissions. Common Configurations Enable RDP and allow it through the firewall on all windows servers created Ansible Variable files inventory.yml Modify hosts associated with the playbook. Assign the IP addressing. winlab.yml Associate \u0026lsquo;roles\u0026rsquo; to the hosts identified in the inventory file. These \u0026lsquo;roles\u0026rsquo; are folders within the directory containing a set of code to configure per host ansible.cfg Tells ansible variable information. In this scenario, identifies to use inventory.yml file. ./group_vars/all.yml Contains specific variable information used within the ./roles/* Ansible code. Prerequisites Linux machine with the following Ansible sudo apt update sudo apt install software-properties-common sudo add-apt-repository --yes --update ppa:ansible/ansible sudo apt install ansible Terraform sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y gnupg software-properties-common curl curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026quot;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026quot; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install terraform Packer curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026quot;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026quot; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install packer Git sudo apt-get install git A Code Interprester I recommend Visual-Studio-Code vSphere Lab Environment vSphere \u0026mdash; Note: This project is using vSphere version 7.0.0 Packer Navigate to Packer Directory First setup Packer environment packer init -upgrade ws2022.pkr.hcl Then apply the Packer configuration to create the Windows Server 2022 Image packer build -timestamp-ui -force -var-file=myvarfile.json ws2022.pkr.hcl This packer execute pulls the newest windows server datacenter 2022 eval .iso from microsoft populates it into the vSphere environment, in the specified datacenter/cluster/host/datastore It then runs commands to: Grab DHCP, Updates the image, Enables SSH, Enables RDP, Configures necessary firewall settings, sets passwords/usernames, \u0026amp; installs VMware Tools to base image Additionally, it will install Chocolatey for packages, notepad++, Edge, \u0026amp; 7-zip After Packer Finishes Roughly an hour depending on processing and internet speed Go into your vSphere and turn the resulting VM into a Template Ensure this mimics the variables you have set in the terraform.tfvars file. This will be our next step. Terraform Navigate to Terraform Directory Setup Terraform Environemnt terraform init Format terraform to ensure it meets criteria required terraform fmt Do a terraform plan to detect any potential errors in code and to see potential end result. Read over this terraform plan Finally, if all the above appears correct, perform a terraform apply terraform apply \u0026hellip; yes This may take awhile, once it is done, double check in vSphere all necessary Virtual Machines were created properly (For me this took 20 minutes to fully complete) Ansible Navigate to Ansible Directory Once you have allowed Terraform to finish its configuraiton: Navigate to your Ansible Directory, cd \u0026lt;path-to-Ansible\u0026gt; Run your ansible playbook ansible-playbook winlab.yml This should run through and detail each change as it plays out References Used/Useful Links I sourced various code and peices of information from the following Git Repositories Stefan Zimmermann GitLab Article Dmitry Teslya GitHub Useful places for refernece Tutorials for Terraform, Packer, and others Hashicorp-Tutorials Ansible Documentation Windows-Modules Terraform Documentation Packer Documentation ","date":"2022-02-14T00:00:00Z","image":"https://cinderblock.github.io/p/terraform-vsphere-windows-server-deployment/vSphereBanner_hu139b00facdfdfbe7c80d150053deb610_784363_120x120_fill_box_smart1_3.png","permalink":"https://cinderblock.github.io/p/terraform-vsphere-windows-server-deployment/","title":"Terraform - vSphere Windows Server Deployment"}]